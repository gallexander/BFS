\documentclass[12pt,a4paper]{article}
\usepackage{termpaper}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{cleveref}
\usepackage{filecontents}


\lstdefinestyle{customc}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  frame=L,
  xleftmargin=\parindent,
  language=C,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily,
  keywordstyle=\bfseries\color{green!40!black},
  commentstyle=\itshape\color{purple!40!black},
  identifierstyle=\color{blue},
  stringstyle=\color{orange},
  tabsize=2
}
\lstset{escapechar=@,style=customc}

%opening
\title{Parallel breadth-first search}
\author{
 \authorname{Alexander Gallauner} \\
 \studentnumber{1026090} \\
 \curriculum{534} \\
 \email{alexander.gallauner@gmail.com}
}

\begin{document}
\maketitle
\begin{abstract}
To have a continuous progress of the performance of processors, it was necessary to concentrate on the development of multicore processors. With that development another problem was raising up. The programs or rather the algorithmic way of thinking have to be changed to get a speed up with multicore processors. It was the job of the developers to split up the work from one core to many cores and to coordinate the communication between these cores. In this report the focus is on algorithms for searching trees or graph data structures and how to parallelize them. To make it specific, we keep our mind on the breadth-first search, because it is easier to parallelize than the depth-first search. First there is a presentation of a sequential algorithm to solve that problem. After that a parallel realization of this sequential algorithm in pseudo code follows. We get an overview of the comparison between the sequential and parallel solution. After that there is a more concrete solution of the parallel algorithm with some implementation details. Additionally we show a history of changes or improvements of the parallel solution and how it influences the speed of the algorithm.
To get a feeling how good the solution is, there is an integration of our solution into a benchmark for supercomputers, the graph500 project. Graph500 establishes a large-scale benchmark for data-intensive supercomputer applications. There exists already a official solution of the benchmark, so we compare the performance of Jupiter, that is the name of the distributed system of the Vienna University of Technology, one time with the own implemented benchmark, but observing the rules of the graph500 project, and another time with the official solution of the graph500 project. With this results it is possible to compare Jupiter with some other supercomputers, listed in the graph500 ranking.
\end{abstract}

\clearpage

\section{The breadth-first search - BFS}
\label{sec:breadth-first search}

\subsection{Sequential BFS}
\label{sec:sequential-bfs}

The sequential BFS (breadth-first search) algorithm isn't difficult to understand. If we have a given tree or alternatively a graph data structure we start with a specific node. That is our root node. We can split the graph or tree into levels, each level is handled in one single iteration of the BFS. So we can say, we start with level 0 at the root node.
Then there starts the first round of the BFS. We take all nodes (root node for the first level) of the current level (that is level 0) and visit all neighbours of the nodes of the current level. All neighbours that have been visited are now the nodes of the next level (in this case that is level 1). The next step is to take all nodes of level 1 and visit all neighbours of these nodes. The BFS algorithm ends if the current level has no neighbours anymore or a specific search key is found. Output of the algorithm is a parent array, which contains parent information of each node. For better understanding the figure below shows a few iterations of the BFS algorithm. Figure is in progress...

\subsection{Parallel BFS}
\label{sec:parallel-bfs}

To make an algorithm parallel, the main problem is to split the work of the sequential algorithm into smaller pieces and assign that pieces of work to the existing processors of the system which are in use. 
The first approach of splitting the BFS is to split the nodes of each level to the processors. Every processor of the system just visit the neighbours of the owned nodes. The problem with this approach is that the communication costs are pretty high, because it's necessary to split the work every round (level) of the BFS algorithm.\\
So the second and also chosen approach is to divide all the existing nodes and corresponding edges of the graph to the processors. With that approach the highest amount of communication is just at the beginning of the algorithm. After that the cores have to communicate in such a way, that they know which nodes are in the next level. That can happen in different ways, but it is important that we have a parent array at the end of the algorithm.  Figure is in progress....

\section{Implementation of the BFS and some improvements}
\label{sec:implementations}

\subsection{Core of the parallel BFS solution}
\label{sec:core}
Here we will present our solution of the core of our parallel BFS algorithm for better understanding in a more detailed pseudo code.\\
Input: Every proc has adjacency buffer of owned nodes and a visited bitmap of all nodes\\
Output: parent array
\begin{lstlisting}
void bfs(){
    char oneChildisVisited = 1;
    while (oneChildisVisited){
        oneChildisVisited = 0;
        for (i = 0; i < size(nodes_owned); i++){
        		if (nodes_owned[i] is visited for the first time) {
				for (j = 0; j < size(neighbours); j++){
					if (neighbours[j] is not visited) {
            					oneChildisVisited = 1;
						set_visited_bitmap(neighbours[j]); // sets the visited bitmap on position of neighbour[j] to 1
						save_parent(nodes_owned[i],neighbours[j]); // save that nodes_owned[i] is parent of neighbours[j]
            				}
				}
			}	
        }
        allreduce(oneChildisVisited); // all procs get the information if there is a neighbour visited at all
        if (oneChildisVisited){
        		allreduce(visited_bitmap, BITWISE_OR); // there has to be a reduction of all visited bitmaps to one
        }
}
\end{lstlisting}
Firstly we need a flag \lstinline{oneChildisVisited}. This flag keeps the information, if there is any new node visited. If not, all procs come to an end.\\
So if there is a new node in the current level, the procs have to iterate the owned nodes and check if there is one of them in the current level and unvisited. If there is a new one in the current level, the proc iterates the neighbours of that node. For each neighbour there is a check if it is set to 1 in the reduced visited bitmap and not visited in a level before. Is that the case the node is set to 1 in the visited bitmap and the owned node is saved as the parent of the neighbour node. After that the procs must reduce the \lstinline{oneChildisVisited} and \lstinline{visited_bitmap} information with the bitwise OR operation.

\subsection{Implementation details of the BFS}

In this section we will show some commands from the pseudo code in \ref{sec:core} and how to implement it in the programming language C with MPI. \\
Let us start with the first condition in the for-loop. 
\begin{lstlisting}
nodes_owned[i] is visited for the first time
\end{lstlisting}
This condition can be translated into
\begin{lstlisting}
position & level[((nodes_owned*my_rank+i) / BITS)] & ~visited[(i / BITS)]
\end{lstlisting}
where 
\begin{lstlisting}
position = (unsigned long) pow(2, (i % BITS));
\end{lstlisting}
We work with bitmaps of type \lstinline{uint64_t}, so it is possible to save true or false (0 or 1) information of 64 nodes in one variable or field. To get the position of a node in a bitmap, we have to calculate two to the power of \(n\) where \(n\) is the number of the specific node modulo the count of possible bits, which is 64 in our case. For instance if our processor has 256 nodes for his own and we want to get the position of node number 90, our \(n\) is 26 and we calculate two to the power of 26. With that calculation we get the value, where the 26th bit is set to one.\\
The next step is to prove that the specific node is also set to one in the level array. The level array is a bitmap of all nodes of the graph and shows us which nodes were visited in the levels before of the BFS. With \lstinline{nodes_owned*my_rank+i} we get the absolute position of the specific node, where \lstinline{i} is just the relative position. So if the absolute position is divided by the count of possible bits we get the correct field of the level array where the specific node is located. Let us show that with the example of node number 90 and with two processors where each processor has 256 nodes. There must be an level array with \(512/64 = 8\) fields and our node with number 90 must be at field number \((256*0+90) / 64 = 1\) where the fields begin with number 0 and the rank of the first processor is 0.\\
The visited array or bitmap gives us the information if a node is already visited or not. So if the bit of a specific node is set to one in the level array and the associated bit in the visited array is zero, we know through the bitwise AND operation \lstinline{&} that the node is visited for the first time. Otherwise if the bit in the visited array is one already, we know that the node was visited in a level or round before.\\
Assuming that the node was not visited before, we iterate over all neighbours of that node. With the condition \lstinline{neighbours[j] is not visited} translated to
\begin{lstlisting}
position & ~level[(buffer[j]/BITS)]
\end{lstlisting}
where
\begin{lstlisting}
position = (unsigned long) pow(2, (buffer[j] % BITS));
\end{lstlisting}
and \lstinline{buffer[j]} is one of the neighbours of the actual node, we check that this neighbour is visited in that level for the first time because of the synchronized level array, unless another node in the same level also visits the that node, because he is also one of his neighbours. Here comes a race condition up, but for us it is enough that at the end the BFS tree represented through the parent array is correct.\\
If the condition evaluates to true we save the actual node as parent of the neighbour with
\begin{lstlisting}
parent_array[buffer[j]] = nodes_owned*my_rank+i+1;
\end{lstlisting}
where \lstinline{parent_array} is an array of type \lstinline{uint64_t} and is as big as the count of nodes of the whole graph. If we save a parent in the parent array, we always calculate \(+1\) to the number of the node to prevent, because the parent array is initialized with zero, that the node with number zero is the parent of all nodes which are not set during the BFS.\\
When all processors finished the actual level, the \lstinline{oneChildisVisited} flag is synchronized with the command \lstinline{allreduce(oneChildisVisited)}, which can be translated to
\begin{lstlisting}
MPI_Allreduce(MPI_IN_PLACE, (void *) &oneChildisVisited, 1, MPI_CHAR, MPI_BOR, MPI_COMM_WORLD);
\end{lstlisting}
If this flag is true, the processors know that there must be a next level or round. So they also synchronize the level array with
\begin{lstlisting}
MPI_Allreduce(MPI_IN_PLACE, (void *)level, (pow(2,SCALE) / BITS), MPI_UNSIGNED_LONG, MPI_BOR, MPI_COMM_WORLD);
\end{lstlisting}
which was \lstinline{allreduce(visited_bitmap, BITWISE_OR)} in pseudo code. After the reduce operation the next level starts.

\subsection{History of our BFS algorithm}

The solution of the algorithm in section \ref{sec:implementations} was not our first approach to solve the BFS problem. Our first idea was to solve it with matrices where each element represents an edge between two nodes. The problem with this approach is that if we have a sparse matrix we have a high waste of memory. So it was our next idea to save only the edges that are existing and to leave out the rest. We decided to store all existing edges in a buffer and to work with an index array to find the edges of each node. So the next important question was if every processor has his own part of the buffer how should each processor share the information which nodes are in the current or next level of the BFS. So we came to the first version of the BFS implementation:

\subsubsection{First version - next level bitmap}

The first version of the algorithm is characterized by a next level bitmap, because every processor stores the information about new visited nodes in a next level bitmap which has a size of \(n\) bits where \(n\) is the count of nodes of the whole graph. So at every round the processors iterate over the owned nodes and check if the actual node in the iteration is in the current level and not visited. If that is the case the algorithm sets all bits of the neighbours of the current node in the next level bitmap to one. At the end of each round there must be a reduce scatter operation between the processors to ensure that every processor gets the level information of the next round.\\
The only problem with that approach is that it is difficult to create a parent array if you just synchronize a next level bitmap between the processors and it is not known who is the parent of the visited nodes. The disadvantage of that procedure is that the reduce scatter operation with a next level array of type \lstinline{uint64_t} takes time and the algorithm becomes inefficient with a higher count of processors.

\subsubsection{Second version - \lstinline{uint64_t} next level array}

In the second version of the algorithm we did not use a next level bitmap, instead we used a next level array of type \lstinline{uint64_t}. At the position where we normally set the bit to one in the next level bitmap, we store the parent of the specific visited node. After the reduce scatter operation of the next level array, where every entry stores the parent of the node, every processor knows the parent of these nodes, which are already visited and which the processor owns.

\subsubsection{Third and current version - visited bitmap}

The basic idea of this approach is that that we do not have a next level bitmap or array, we always keep a bitmap of all visited nodes in all levels synchronized. All processors know which node is already visited and which not and the parent of each node can be correctly stored at each processor. It is important that the bitmap of all visited nodes is synchronized at each end of the round. Section \ref{sec:implementations} gives more information about this approach.

If you want to study the core of this algorithm in a more detailed way, you can check the appendix of this paper. There is the source code \ref{sec:sourcecode} in fully length.



\section{Testing the algorithm on Jupiter}
\label{sec:testing}

Here are some results of testing the solution on jupiter, there will be soon a better illustration of the results: \\
\\
SCALE 22\\
EDGEFACTOR 4\\
\\
mpirun -host jupiter1 -np 1 ./par\\
Time for generating and scattering: 71.516787\\
Time for bfs searching: 14.322063\\
\\
mpirun -host jupiter1 -np 2 ./par\\
Time for generating and scattering: 71.074316\\
Time for bfs searching: 7.191137\\
\\
mpirun -host jupiter1 -np 4 ./par\\
Time for generating and scattering: 71.667024\\
Time for bfs searching: 3.624339\\
\\
mpirun -host jupiter1 -np 8 ./par\\
Time for generating and scattering: 71.868302\\
Time for bfs searching: 1.847186\\
\\
mpirun -host jupiter1 -np 16 ./par\\
Time for generating and scattering: 71.795941\\
Time for bfs searching: 0.970461\\
\\
mpirun -host jupiter1 -np 16 -host jupiter2 -np 16 ./par\\
Time for generating and scattering: 78.301455\\
Time for bfs searching: 0.718315\\
\\
mpirun -host jupiter1 -np 16 -host jupiter2 -np 16 -host jupiter3 -np 16 -host jupiter4 -np 16 ./par\\
Time for generating and scattering: 78.680866\\
Time for bfs searching: 0.817814\\
\\
SCALE 23\\
EDGEFACTOR 4\\
\\
mpirun -host jupiter1 -np 1 ./par\\
Time for generating and scattering: 151.001563\\
Time for bfs searching: 32.189217\\
\\
mpirun -host jupiter1 -np 2 ./par\\
Time for generating and scattering: 150.288253\\
Time for bfs searching: 14.429639\\
\\
mpirun -host jupiter1 -np 4 ./par\\
Time for generating and scattering: 149.650995\\
Time for bfs searching: 7.580528\\
\\
mpirun -host jupiter1 -np 8 ./par\\
Time for generating and scattering: 150.740900\\
Time for bfs searching: 3.720919\\
\\
mpirun -host jupiter1 -np 16 ./par\\
Time for generating and scattering: 150.239736\\
Time for bfs searching: 1.960265\\
\\
mpirun -host jupiter1 -np 16 -host jupiter2 -np 16 ./par\\
Time for generating and scattering: 164.370905\\
Time for bfs searching: 1.317743\\
\\
mpirun -host jupiter1 -np 16 -host jupiter2 -np 16 -host jupiter3 -np 16 -host jupiter4 -np 16 ./par\\
Time for generating and scattering: 163.695266\\
Time for bfs searching: 1.315105\\
\\
SCALE 25\\
EDGEFACTOR 4\\
\\
mpirun -host jupiter1 -np 1 ./par\\
Time for generating and scattering: 209.442903\\
Time for bfs searching: 124.501475\\
\\
mpirun -host jupiter1 -np 2 ./par\\
Time for generating and scattering: 206.821771\\
Time for bfs searching: 62.673911\\
\\
mpirun -host jupiter1 -np 4 ./par\\
Time for generating and scattering: 204.477917\\
Time for bfs searching: 32.574306\\
\\
mpirun -host jupiter1 -np 8 ./par\\
Time for generating and scattering: 163.052962\\
Time for bfs searching: 16.547304\\
\\
mpirun -host jupiter1 -np 16 ./par\\
Time for generating and scattering: 206.160913\\
Time for bfs searching: 9.362862\\
\\
mpirun -host jupiter1 -np 16 -host jupiter2 -np 16 ./par\\
Time for generating and scattering: 164.124661\\
Time for bfs searching: 5.341782\\
\\
mpirun -host jupiter1 -np 16 -host jupiter2 -np 16 -host jupiter3 -np 16 -host jupiter4 -np 16 ./par\\
Time for generating and scattering: 165.615725\\
Time for bfs searching: 3.986594\\

%\begin{figure}[htbp]
%	\centering
%	\includegraphics[width=15cm]{benchmark}%
%	\caption{Ergebnisse auf Jupiter}%
%	\label{fig:benchmark}
%\end{figure}

\section{The graph500 project and my own realisation}
\label{sec:graph500}

In this section will be an implementation of the parallel BFS in MPI. The programming language is C, MPI is used to ensure the parallel solution. For more information about MPI read Rauber 2013 (reference not working :( , in progress)\\
Because it would be too costly to type in the node data for a scale of \(2^{32}\) or higher, the first part of the program is to generate nodes. For the data generation we used a Kronecker generator. Later in this paper there will be a section of this Kronecker generator, which creates natural graphs. In an own program the kronecker generator generates an edge list, containing start vertices and end vertices. These edge list will be written into a file, so the graph don't have to be generated each time for new. For generation the following void function will be used:

\begin{lstlisting}
generate_graph(SCALE, EDGEFACTOR, initiator, startVertex, endVertex);
\end{lstlisting}

In this line the Kronecker generator fills the \lstinline{startVertex[i]} and \lstinline{endVertex[i]} arrays with random numbers of nodes. Each pair of \lstinline{startVertex[i]} and \lstinline{endVertex[i]}, for \( 0 < i < N, N = count of nodes \) is representing an edge of the graph. The \lstinline{SCALE} parameter gives the generator the size of the graph and the \lstinline{EDGEFACTOR} the ratio of the count of the edges to the count of the nodes. The \lstinline{initiator} is a 2x2 map, filled with probabilities, which influences the functioning of the Kronecker generator. So after the generation our root proc reads the graph data from the file and brings the information in a more computeable form.

\begin{lstlisting}
create_node_edge_lists(nodes, edges, startVertex, endVertex, node_edge_list, count_edges_per_node);
\end{lstlisting}
Firstly the arrays of vertexes are converted to an array of nodes, where each entry of the array points to a list of nodes, which are neighbours of this specific entry.
\begin{lstlisting}
buffer = lists_to_buffer(&buffer_size, node_edge_list, count_edges_per_node, 0, nodes-1);
\end{lstlisting}
Secondly that array of nodes, where each entry points to a list, is transformed to a buffer, where the neighbours of the node 1 are at the beginning of the buffer, then the neighbours of the node 2 follow the neighbours of node 1, and so on.
The next step is to calculate the amount of edges each proc of the system is receiving, so that the procs can allocate the necessary memory for their part of the buffer and scattering it from the root proc to all procs.  So far we just handled functions, which were only processed by the root proc, but from now the functions in the future get called by the root AND all other procs. \\
After that calling the function
\begin{lstlisting}
MPI_Scatterv((void *) buffer, buffer_send_counts, buffer_displs, MPI_UINT64_T, (void *) buffer_recvbuf, buffer_recv_size, MPI_UINT64_T, 0, MPI_COMM_WORLD);
\end{lstlisting}
the \lstinline{buffer} is scattered to all procs, which have already allocated the necessary space. \lstinline{buffer_send_counts} gives the information, how many units each proc gets. \lstinline{buffer_displs} is a vector, whichs shows the starting index of the buffer for each proc, so the right part of the entire buffer is scattered for each proc. \lstinline{buffer_recvbuf} ensures that every proc has a buffer to store the received information.\\
As we already above-mentioned at the explanation of our approach of the parallel BFS there has to be a thinking in levels and there has to be a communcation, which procs must work at the next round/level. For instance at the beginning the root node is in the first level (level 0). So just the proc, which is responsible for the root node, is working in the first round. The next step is that the neighbours of the root node have their turn. All procs, which own one node of the second level, have to work in the next level. With
\begin{lstlisting}
MPI_Scatter((void *)level,nodes / BITS / procs,MPI_UNSIGNED_LONG, (void *)level_recvbuf, nodes / BITS / procs, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);
\end{lstlisting}
the nodes of the first round get scattered. \lstinline{level} is a bitmap of type \lstinline{unsigned long} with the nodes of the current round set to 1. \lstinline{nodes / BITS / procs} says how many units of type \lstinline{unsigned long} every proc (including the root node) is receiving. \\
At the next instruction the procs (also the root proc) comes to the most relevant function of the algorithm.
\begin{lstlisting}
bfs(level_recvbuf, buffer_recvbuf, buffer_recv_size, count_edges_per_node_recvbuf, (nodes / procs), procs);
\end{lstlisting}
Here is the beginning of the basic algorithm of our parallel BFS algorithm. All steps so far were a preparation. The edge list was transformed into a better computeable form and the root node had to scatter some important data to the other procs of the system.

\section{Comparing with the official solution of graph500}
\label{sec:Comparing}

\section{Ranking of Jupiter}
\label{sec:ranking}

\section{Why no parallel realisation of depth-first search?}
\label{sec:depth-first search}

\section{Summary}
\label{sec:summary}

\clearpage

\nocite{*}
\bibliographystyle{abbrv}
\bibliography{references}

\appendix
\section{Source Code}
\label{sec:sourcecode}
\lstinputlisting[caption={Parallel BFS algorithm},label=bfs_par]{../Source/bfs_par.c}
\clearpage
\lstinputlisting[caption={Kronecker Generator},label=kronecker_generator]{../Source/kronecker_generator.c}
\clearpage
\lstinputlisting[caption={project.h},label=project]{../Source/project.h}

\clearpage


\end{document}