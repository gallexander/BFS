\documentclass[12pt,a4paper]{article}
\usepackage{termpaper}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{cleveref}
\usepackage{filecontents}


\lstdefinestyle{customc}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  frame=L,
  xleftmargin=\parindent,
  language=C,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily,
  keywordstyle=\bfseries\color{green!40!black},
  commentstyle=\itshape\color{purple!40!black},
  identifierstyle=\color{blue},
  stringstyle=\color{orange},
  tabsize=2
}
\lstset{escapechar=@,style=customc}

%opening
\title{Parallel breadth-first search}
\author{
 \authorname{Alexander Gallauner} \\
 \studentnumber{1026090} \\
 \curriculum{534} \\
 \email{alexander.gallauner@gmail.com}
}

\begin{document}
\maketitle
\begin{abstract}
Some years ago, to have a continuous progress of the performance of processors, it was necessary to stop trying to increase the clock rate of CPUs, and to concentrate on the development of multicore processors. But with this development another problem was raising up. The programs or rather the algorithmic problems have to be changed to get a speed up with multicore processors. It was the job of the developers to split the work from one core to many cores and to coordinate the communication between these cores. In this scientific paper I will concentrate on algorithms for searching tree or graph data structures and how to parallelize them. To make it specific, I will keep my mind on the breadth-first search, because it is easier to parallelize than the depth-first search. First I will show a sequential algorithm to solve this problem. After that my focus is on a parallel realisation of this sequential algorithm. In this chapter I will demonstrate some improvements to the parallel algorithm to reach a better speed up.
To get a feeling how good the solution is, I will populate my solution into a benchmark for supercomputers, the graph500 project. Graph500 establishes a large-scale benchmark for data-intensive supercomputer applications. There exists already a official solution of the benchmark, so I can compare the performance of Jupiter, that is the name of the distributed system of the Vienna University of Technology, one time with my own implemented benchmark, but observing the rules of the graph500 project, and another time with the official solution of the graph500 project. Another thing is that I compare Jupiter with other supercomputers, which are listed in the graph500 ranking.
\end{abstract}

\clearpage

\section{The breadth-first search - BFS}
\label{sec:breadth-first search}

\subsection{Sequential BFS}
\label{sec:sequential-bfs}

The sequential BFS (breadth-first search) algorithm isn't difficult to understand. If you have a given tree or alternatively a graph data structure, you start with a specific node. This is our root node. We can split the graph or tree into levels, each level is handled in one single iteration of the BFS. So we can say, we start with level 0 at the root node.
Then there starts the first round of the BFS. We take all nodes of the current level (that is level 0) and visit all neighbours of the nodes of the current level. All neighbours that have been visited are now the nodes of the next level (in this case it's level 1). The next step is to take all nodes of level 1 and visit all neighbours of these nodes. The BFS algorithm ends if the current level has no neighbours anymore or a specific search key is found. For better understanding the figure below shows a few iterations of the BFS algorithm. Figure is in progress...

\subsection{Parallel BFS}
\label{sec:parallel-bfs}

To make an algorithm parallel, the main problem is to split the work of the sequential algorithm into smaller pieces and assign that pieces of work to the existing cores of the system, which are in use. 
The first approach of splitting the BFS is to split the nodes of each level and every core of the system just visit the neighbours of the owned nodes. The problem with this approach is that the communication expense is pretty high, because it's necessary to split the work every round (level) of the BFS algorithm.\\
So the second and our used approach is to divide all the existing nodes and corresponding edges of the graph to the processors. With this approach the highest expense of communication is just at the beginning of the algorithm. After the processors just have to communicate, which node is in the current level. For instance if I have the root node at proc 0, the whole work of level 0 is just at the processor with the number 0. But now proc 0 has to tell the other processors, which nodes he has visited. So these processors have to prove if they have one of these visited nodes stored in their node list. If they have, it's their turn for the next level.

\section{Implementation of the BFS and some improvements}
\label{sec:implementations}

\subsection{Parallel Implementation}
\label{sec:parallel-impl}

In this section will be an implementation of the parallel BFS in MPI. The programming language is C, MPI is used to ensure the parallel solution. For more information about MPI read Rauber 2013 (reference not working :( , in progress)\\
Because it would be too costly to type in the node data for a scale of \(2^{32}\) or higher, the first part of the program is to generate nodes. For the data generation we used a Kronecker generator. Later in this paper there will be a section of this Kronecker generator, which creates natural graphs. In an own program the kronecker generator generates an edge list, containing start vertices and end vertices. These edge list will be written into a file, so the graph don't have to be generated each time for new. For generation the following void function will be used:

\begin{lstlisting}
generate_graph(SCALE, EDGEFACTOR, initiator, startVertex, endVertex);
\end{lstlisting}

In this line the Kronecker generator fills the \lstinline{startVertex[i]} and \lstinline{endVertex[i]} arrays with random numbers of nodes. Each pair of \lstinline{startVertex[i]} and \lstinline{endVertex[i]}, for \( 0 < i < N, N = count of nodes \) is representing an edge of the graph. The \lstinline{SCALE} parameter gives the generator the size of the graph and the \lstinline{EDGEFACTOR} the ratio of the count of the edges to the count of the nodes. The \lstinline{initiator} is a 2x2 map, filled with probabilities, which influences the functioning of the Kronecker generator. So after the generation our root proc reads the graph data from the file and brings the information in a more computeable form.

\begin{lstlisting}
create_node_edge_lists(nodes, edges, startVertex, endVertex, node_edge_list, count_edges_per_node);
\end{lstlisting}
Firstly the arrays of vertexes are converted to an array of nodes, where each entry of the array points to a list of nodes, which are neighbours of this specific entry.
\begin{lstlisting}
buffer = lists_to_buffer(&buffer_size, node_edge_list, count_edges_per_node, 0, nodes-1);
\end{lstlisting}
Secondly that array of nodes, where each entry points to a list, is transformed to a buffer, where the neighbours of the node 1 are at the beginning of the buffer, then the neighbours of the node 2 follow the neighbours of node 1, and so on.
The next step is to calculate the amount of edges each proc of the system is receiving, so that the procs can allocate the necessary memory for their part of the buffer and scattering it from the root proc to all procs.  So far we just handled functions, which were only processed by the root proc, but from now the functions in the future get called by the root AND all other procs. \\
After that calling the function
\begin{lstlisting}
MPI_Scatterv((void *) buffer, buffer_send_counts, buffer_displs, MPI_UINT64_T, (void *) buffer_recvbuf, buffer_recv_size, MPI_UINT64_T, 0, MPI_COMM_WORLD);
\end{lstlisting}
the \lstinline{buffer} is scattered to all procs, which have already allocated the necessary space. \lstinline{buffer_send_counts} gives the information, how many units each proc gets. \lstinline{buffer_displs} is a vector, whichs shows the starting index of the buffer for each proc, so the right part of the entire buffer is scattered for each proc. \lstinline{buffer_recvbuf} ensures that every proc has a buffer to store the received information.\\
As we already above-mentioned at the explanation of our approach of the parallel BFS there has to be a thinking in levels and there has to be a communcation, which procs must work at the next round/level. For instance at the beginning the root node is in the first level (level 0). So just the proc, which is responsible for the root node, is working in the first round. The next step is that the neighbours of the root node have their turn. All procs, which own one node of the second level, have to work in the next level. With
\begin{lstlisting}
MPI_Scatter((void *)level,nodes / BITS / procs,MPI_UNSIGNED_LONG, (void *)level_recvbuf, nodes / BITS / procs, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);
\end{lstlisting}
the nodes of the first round get scattered. \lstinline{level} is a bitmap of type \lstinline{unsigned long} with the nodes of the current round set to 1. \lstinline{nodes / BITS / procs} says how many units of type \lstinline{unsigned long} every proc (including the root node) is receiving. \\
At the next instruction the procs (also the root proc) comes to the most relevant function of the algorithm.
\begin{lstlisting}
bfs(level_recvbuf, buffer_recvbuf, buffer_recv_size, count_edges_per_node_recvbuf, (nodes / procs), procs);
\end{lstlisting}
Here is the beginning of the basic algorithm of our parallel BFS algorithm. All steps so far were a preparation. The edge list was transformed into a better computeable form and the root node had to scatter some important data to the other procs of the system.
\subsubsection{Core of the parallel BFS solution}
Here we will present the core of our parallel BFS algorithm for better understanding in pseudo code.
\begin{lstlisting}
void bfs(){
    char oneChildisVisited = 1;
    while (oneChildisVisited){
        oneChildisVisited = 0;
        for (i = 0; i < size(nodes_owned); i++){
        		if nodes_owned[i] is in current level{
			for (j = 0; j < size(neighbours); j++){
				if neighbours[j] is not visited {
            				oneChildisVisited = 1;
					set_nextlevel_bitmap(i); // sets the bitmap on position of neighbour[i] to 1
            			}
			}
		}
        }
        allreduce(oneChildisVisited); // all procs get the information if there is a neighbour visited at all
        if (oneChildisVisited){
        		alltoall(next_level); // every proc has to send the bitmap of all visited neighbours to the other procs
        }
}
\end{lstlisting}
Firstly we need a flag \lstinline{oneChildisVisited}. This flag keeps the information, if there is visited a new node in any proc. If not, all procs come to an end.\\
So if there is a new node in the current level, the procs have to iterate the owned nodes and check if there is one of them in the current level. If one is in the current level, the proc iterates the neighbours of this owned node. If one neighbour is not visited already, this node becomes a new node of the next level. After that the procs must exchange the data \lstinline{oneChildisVisited} and \lstinline{next_level}. \\
If you want to study the core of this algorithm in a more detailed way, you can check the appendix of this paper. There is the source code \ref{sec:sourcecode} in full length.
\section{Testing the algorithm on Jupiter}
\label{sec:testing}

Here are some results of testing the solution on jupiter, there will be soon a better illustration of the results: \\
\\
SCALE 22\\
EDGEFACTOR 4\\
\\
mpirun -host jupiter1 -np 1 ./par\\
Time for generating and scattering: 71.516787\\
Time for bfs searching: 14.322063\\
\\
mpirun -host jupiter1 -np 2 ./par\\
Time for generating and scattering: 71.074316\\
Time for bfs searching: 7.191137\\
\\
mpirun -host jupiter1 -np 4 ./par\\
Time for generating and scattering: 71.667024\\
Time for bfs searching: 3.624339\\
\\
mpirun -host jupiter1 -np 8 ./par\\
Time for generating and scattering: 71.868302\\
Time for bfs searching: 1.847186\\
\\
mpirun -host jupiter1 -np 16 ./par\\
Time for generating and scattering: 71.795941\\
Time for bfs searching: 0.970461\\
\\
mpirun -host jupiter1 -np 16 -host jupiter2 -np 16 ./par\\
Time for generating and scattering: 78.301455\\
Time for bfs searching: 0.718315\\
\\
mpirun -host jupiter1 -np 16 -host jupiter2 -np 16 -host jupiter3 -np 16 -host jupiter4 -np 16 ./par\\
Time for generating and scattering: 78.680866\\
Time for bfs searching: 0.817814\\
\\
SCALE 23\\
EDGEFACTOR 4\\
\\
mpirun -host jupiter1 -np 1 ./par\\
Time for generating and scattering: 151.001563\\
Time for bfs searching: 32.189217\\
\\
mpirun -host jupiter1 -np 2 ./par\\
Time for generating and scattering: 150.288253\\
Time for bfs searching: 14.429639\\
\\
mpirun -host jupiter1 -np 4 ./par\\
Time for generating and scattering: 149.650995\\
Time for bfs searching: 7.580528\\
\\
mpirun -host jupiter1 -np 8 ./par\\
Time for generating and scattering: 150.740900\\
Time for bfs searching: 3.720919\\
\\
mpirun -host jupiter1 -np 16 ./par\\
Time for generating and scattering: 150.239736\\
Time for bfs searching: 1.960265\\
\\
mpirun -host jupiter1 -np 16 -host jupiter2 -np 16 ./par\\
Time for generating and scattering: 164.370905\\
Time for bfs searching: 1.317743\\
\\
mpirun -host jupiter1 -np 16 -host jupiter2 -np 16 -host jupiter3 -np 16 -host jupiter4 -np 16 ./par\\
Time for generating and scattering: 163.695266\\
Time for bfs searching: 1.315105\\
\\
SCALE 25\\
EDGEFACTOR 4\\
\\
mpirun -host jupiter1 -np 1 ./par\\
Time for generating and scattering: 209.442903\\
Time for bfs searching: 124.501475\\
\\
mpirun -host jupiter1 -np 2 ./par\\
Time for generating and scattering: 206.821771\\
Time for bfs searching: 62.673911\\
\\
mpirun -host jupiter1 -np 4 ./par\\
Time for generating and scattering: 204.477917\\
Time for bfs searching: 32.574306\\
\\
mpirun -host jupiter1 -np 8 ./par\\
Time for generating and scattering: 163.052962\\
Time for bfs searching: 16.547304\\
\\
mpirun -host jupiter1 -np 16 ./par\\
Time for generating and scattering: 206.160913\\
Time for bfs searching: 9.362862\\
\\
mpirun -host jupiter1 -np 16 -host jupiter2 -np 16 ./par\\
Time for generating and scattering: 164.124661\\
Time for bfs searching: 5.341782\\
\\
mpirun -host jupiter1 -np 16 -host jupiter2 -np 16 -host jupiter3 -np 16 -host jupiter4 -np 16 ./par\\
Time for generating and scattering: 165.615725\\
Time for bfs searching: 3.986594\\

%\begin{figure}[htbp]
%	\centering
%	\includegraphics[width=15cm]{benchmark}%
%	\caption{Ergebnisse auf Jupiter}%
%	\label{fig:benchmark}
%\end{figure}

\section{The graph500 project and my own realisation}
\label{sec:graph500}

\section{Comparing with the official solution of graph500}
\label{sec:Comparing}

\section{Ranking of Jupiter}
\label{sec:ranking}

\section{Why no parallel realisation of depth-first search?}
\label{sec:depth-first search}

\section{Summary}
\label{sec:summary}

\clearpage

\nocite{*}
\bibliographystyle{abbrv}
\bibliography{references}

\appendix
\section{Source Code}
\label{sec:sourcecode}
\lstinputlisting[caption={Parallel BFS algorithm},label=bfs_par]{../Source/bfs_par.c}
\clearpage
\lstinputlisting[caption={Kronecker Generator},label=kronecker_generator]{../Source/kronecker_generator.c}
\clearpage
\lstinputlisting[caption={project.h},label=project]{../Source/project.h}

\clearpage


\end{document}