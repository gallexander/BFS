\documentclass[11pt,a4paper]{article}
\usepackage{termpaper}
\usepackage[utf8]{inputenc} 
\usepackage[ngerman]{babel}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{cleveref}
\usepackage{filecontents}
\usepackage{url}
\usepackage[labelfont=bf,textfont=it]{caption}
\usepackage{courier}
\usepackage{subcaption}
\usepackage{multirow}

\definecolor{lstcolor}{rgb}{0.95,0.95,0.95}

\lstdefinestyle{customc}{
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{black}\bfseries\underbar,
	identifierstyle=,
	commentstyle=\color{white},
	stringstyle=\ttfamily,
	showstringspaces=false,
  	tabsize=2,
	mathescape=true,
	numbers=left,
	numberstyle=\tiny,
	numbersep=10pt,
	captionpos=b,
	frame=trBL,
	breaklines=true
}
\lstset{style=customc}
\renewcommand{\lstlistingname}{Algorithmus}

%opening
\title{Parallele Breitensuche}
\author{
 \authorname{Alexander Gallauner} \\
 \studentnumber{1026090} \\
 \curriculum{534} \\
 \email{alexander.gallauner@gmail.com}
}

\begin{document}
\maketitle
\begin{abstract}
In dieser Arbeit beschäftigen wir uns mit dem Parallelisieren eines bekannten Algorithmus, der Breitensuche (BFS - \textit{Breadth-First Search}). Dabei handelt es sich um einen Suchalgorithmus für Graphen, der in vielen Fragestellungen der Graphentheorie involviert ist. Wir werden uns auf die Parallelisierung dieses Verfahrens auf Rechner mit physikalisch verteiltem Speicher (DMM - \textit{distributed memory machine}) beschränken und mehrere Wege aufweisen, wie man diese Parallelisierung realisieren kann. Dies ist keine leichte Aufgabe, da parallele Abläufe in der sequentiellen Breitensuche nicht sofort ersichtlich sind und ein guter Kompromiss zwischen Speicherverbrauch und Kommunikationsaufwand zwischen Prozessoren gefunden werden muss. Weiters ist es herausfordernd, die Arbeit gleichmäßig auf die verfügbaren Prozessoren aufzuteilen.\\
Es werden verschiedene parallele Algorithmen vorgestellt und die Implementierungen dieser Algorithmen anhand verschiedener Kriterien wie Laufzeit, Speedup und Kosten analysiert. Außerdem wird erklärt, welche Voraussetzungen gelten müssen, damit ein bestimmter Algorithmus überhaupt Sinn macht. Die Implementierung findet in der Programmiersprache C statt und zur Kommunikation der Prozessoren beziehungsweise der Knoten wird Open MPI verwendet, das eine Open-Source Implementierung des Message Passing Interface Standards (MPI) bereitstellt. Um die Kommunikation zwischen den Knoten einer  DMM zu reduzieren, haben wir auch eine Hybrid-Variante entwickelt. Die Implementierung dieser Hybrid-Variante verwendet eine Kombination aus MPI und OpenMP, wobei OpenMP eine Programmierschnittstelle für die Programmierung von Parallelrechnern mit gemeinsamen Adressraum darstellt. (Beispiel Speedup)\\
Zusätzlich wird das Projekt Graph500 vorgestellt. Graph500 ist ein Benchmark für Supercomputer, die datenintensive Anwendungen ausführen. Da die Breitensuche bei groß angelegten Graphen ein ebenfalls daten- und rechenintensives Problem darstellt, steht diese Suche im Mittelpunkt des Graph500 Projektes. So wie die Implementierungen der parallelen Breitensuche wird auch unsere Implementierung des Graph500 Projektes auf dem Supercomputer Jupiter, ein Rechner der TU Wien mit physikalisch verteiltem Speicher, ausgeführt und analysiert. Innerhalb dieses Projektes wird die Leistung in der Maßeinheit TEPS (\textit{traversed edges per second}) gemessen, welche Auskunft gibt, wie viele Kanten pro Sekunde innerhalb des Graphen besucht werden und wo sich Jupiter anhand des Benchmarks in der Graph500 Rangliste der Supercomputer einordnen würde.
\end{abstract}
\clearpage
\section{Einleitung}
\label{sec:einleitung}
Graphabstraktionen spielen in vielen wissenschaftlichen aber auch alltäglichen Gebieten eine wesentliche Rolle. Viele algorithmische Probleme können auf Graphen zurückgeführt werden, aber auch umgekehrt basiert die Lösung graphentheoretischer Probleme auf Algorithmen. Das Problem der Suche nach dem kürzesten Weg zwischen zwei Orten in einem Straßennetz kann durch einen Graph abstrahiert und mit Hilfe dieses Graphs gelöst werden. Ein Algorithmus, der den kürzesten Weg in einem ungewichteten Graph findet und Thema dieser Bachelorarbeit ist, ist die Breitensuche.\\
Vor über einem halben Jahrhundert wurde der erste Algorithmus, der den Graphen im Prinzip der Breitensuche traversiert, von Moore~\cite{moore}, während er sich mit der Findung von Pfaden in Labyrinthen beschäftigte, untersucht und herausgebracht. Fast zur gleichen Zeit und unabhängig von der Arbeit von Moore untersuchte Lee~\cite{lee} den selben Algorithmus, aber in Bezug auf das Verlegen von Drähten auf einer Platine und die damit verbundene automatische Herstellung von Platinen. Bevor wir mehr auf die Breitensuche eingehen, werden wir zuerst die Eigenschaften eines Graphen genauer beschreiben und die einzelnen Bestandteile erklären und veranschaulichen.
\subsection{Graphdefinition}
Ein Graph in der Graphentheorie ist eine abstrakte Struktur, die eine Menge von Objekten und die Verbindungen zwischen diesen Objekten repräsentiert. Genauer ausgedrückt besteht ein Graph \(G = (V, E)\), wie von Drmota, Gittenberger, Karigl und Panholzer in~\cite{mathematik} beschrieben,  aus einer endlichen Knotenmenge \(V = V(G)\) und einer endlichen Kantenmenge \(E = E(G)\). Dabei kann eine Kante gerichtet oder ungerichtet sein. Falls alle Kanten gerichtet sind, spricht man auch von einen gerichteten Graphen, andererseits von einem ungerichteten Graphen. In dieser Arbeit arbeiten wir ausschließlich mit ungerichteten Graphen, das heißt jede Kante \(e \in E(G)\) entspricht einem ungeordnetem Paar \(e = \{ v_{1},v_{2} \} = v_{1}v_{2}\) von zwei Knoten \(v_{1}, v_{2} \in V(G)\).
\begin{figure}[h]
 	\centering
	\includegraphics[width=0.5\textwidth]{graph}
 	\caption{Beispiel eines ungerichteten Graphen mit einem Startknoten und einer Schlinge.}
	\label{fig:graph}
\end{figure}
Abbildung~\ref{fig:graph} zeigt ein Beispiel für einen ungerichteten Graphen. Eingezeichnet ist bereits ein Startknoten, schwarzer Knoten, da jede Breitensuche neben dem Graphen einen Start- beziehungsweise Wurzelknoten als Eingabeparameter erwartet. Zusätzlich ist im Graphen eine Schlinge ersichtlich, es sind also auch Kanten zugelassen, bei denen Anfangsknoten gleich Endknoten sind, was innerhalb einer Implementierung der Breitensuche berücksichtigt werden muss. Außerdem kann es sein, dass der Graph nicht zusammenhängend ist und es daher keine Kante zu mindestens einem Knoten gibt. Liegt so ein Graph vor, kann es sein, dass bestimmte Knoten ausgehend vom Startknoten durch die Breitensuche nicht erreicht werden können.\\
Um die Breitensuche zu konkretisieren beziehungsweise besser beschreiben zu können, wird noch der Begriff des Weges und der Distanz zwischen zwei Knoten spezifiziert. Eine Kantenfolge \(e_{1}, e_{2}, ..., e_{k} \in E(G)\) in einem ungerichteten Graphen \(G\) heißt \textit{Weg}, wenn alle Knoten, die durch diese Kantenfolge durchlaufen werden, voneinander verschieden sind. Während die Länge des Weges mit der Anzahl der Kanten zwischen zwei Knoten gleichzusetzen ist, bezeichnet man die \textit{Distanz} zweier Knoten  als den kürzesten Weg zwischen diesen. Das Prinzip der Breitensuche impliziert nun, dass zuerst die Knoten mit der Distanz \(d\) besucht werden und erst in Folge die Knoten mit der Distanz \(d+1\). Dadurch wird der Graph durch die Breitensuche in Levels unterteilt, wobei ein \textit{Level} eine Menge von Knoten definiert, die die selbe Distanz zum Startknoten aufweisen, was in Abbildung~\ref{fig:graph_level} ersichtlich ist. Dabei kann man auch sagen, dass der Knoten die Distanz \(d\) aufweist beziehungsweise zum Level \(d\) gehört. Die größte Distanz zwischen zwei Knoten in einem Graphen \(G\) wird auch als Durchmesser \(D(G)\) (\textit{diameter}) bezeichnet.\\
\begin{figure}[h]
 	\centering
	\includegraphics[width=0.5\textwidth]{graph_level}
 	\caption{Nach der Breitensuche ist der Graph in Levels unterteilt. Dies ist ersichtlich an den Distanzen, die in den Knoten eingezeichnet sind. Der graue Knoten entspricht einem nicht besuchten Knoten.}
	\label{fig:graph_level}
\end{figure}
Ausgehend vom Startknoten wurde die Breitensuche auf den Graph von Abbildung~\ref{fig:graph} angewendet. Dadurch wird der Graph in die einzelnen Levels unterteilt, welche aus der Beschriftung der Knoten ersichtlich sind. Der grau hinterlegte Knoten entspricht einem Knoten, der durch die Breitensuche nicht erreicht wurde. Der Durchmesser ist gleich dem höchstem Level im Graphen und ist in diesem Fall \(3\).
Es ist auch möglich andere Informationen als die Distanz in Bezug auf einen Knoten abzuspeichern. Dies wird von vielen unterschiedlichen Applikationen, die die Breitensuche anwenden, erwartet beziehungsweise gefordert. Für unsere Implementierungen ist es Voraussetzung, dass der Vaterknoten für jeden besuchten Knoten abgespeichert wird, damit man am Ende der Breitensuche einen Spannbaum der besuchten Knoten liefern kann. Diese Voraussetzung hat auch unsere Implementierungen stark beeinflusst, da es das Problem der Breitensuche, im Speziellen für den parallelen Fall, aufwendiger macht. Aus Performancezwecken und auch als Beitrag zur Einfachheit und Parallelität sollten zusätzlichen Informationen oder Berechnungen, wenn möglich, beim erstmaligen Besuchen des Knoten durchgeführt werden.
\subsection{Sequentielle Breitensuche}
Nachdem die grundlegenden Bestandteile und Eigenschaften eines Graphen geklärt sind, widmen wir uns weiters dem eigentlichen Thema dieser Arbeit, nämlich der Breitensuche. Auch wenn das Grundkonzept der Breitensuche schon erklärt wurde, zeigt Algorithmus~\ref{fifo} eine klassische sequentielle Variante von Cormen, Leiserson, Rivest und Stein \cite{cormen_introduction_2009}, welche eine Warteschlange (\textit{queue}) verwendet, die im FIFO (\textit{first in first out}) Prinzip arbeitet. Diese Variante der Implementierung gehört zu einer der einfachsten der Breitensuche.
\begin{lstlisting}[caption={Klassische Variante der Breitensuche unter Verwendung einer FIFO Warteschlange als Datenstruktur. Wird auf einem Graph \(G\) mit Startknoten \(v_{0} \in V(G)\) angewandt. Der Algorithmus bestimmt die Distanz und den Vaterknoten von denjenigen Knoten, die ausgehend vom Startknoten erreichbar sind.},label=fifo]
$\textbf{SERIAL-BFS-QUEUE}$(G,$s$)
	$\textbf{for}$ each vertex u $\in$ V(G) - {$s$}
		u.d = $\infty$
		u.$\pi$ = NIL
	$s$.d = 0
	$s$.$\pi$ = s
	Q = {$s$}
	$\textbf{while}$ Q $\neq$ 0
		u = DEQUEUE(Q)
		$\textbf{for}$ each v $\in$ G.Adj[u]
			$\textbf{if}$ v.d == $\infty$
				v.d = u.d + 1
				v.$\pi$ = u
				ENQUEUE(Q,v)
\end{lstlisting}
Zu Beginn von Algorithmus~\ref{fifo} werden alle Knoten, die in \(V(G)\) enthalten sind, in den Zeilen 2-4 initialisiert. Ein Knoten \(u \in V\) besitzt die Attribute \lstinline{u.d} für die Distanz und \lstinline{u.$\pi$} für den Vaterknoten. Da zu Beginn noch keine Knoten besucht wurden, wird die Distanz \lstinline{u.d} auf \lstinline{$\infty$} und der Vaterknoten \lstinline{u.$\pi$} auf \lstinline{NIL} gesetzt. Das hat auch folgenden Grund, dass Knoten, die nicht durch die Breitensuche erreicht werden, keine Vaterknoten besitzen, \lstinline{u.$\pi$ = NIL}, und keine gültige Distanz, \lstinline{u.d = $\infty$}, ausgehend vom Startknoten aufweisen.\\
Zeile 5 weist dem Attribut Distanz des Startknotens \(s\) den Wert 0 zu. In Zeile 6 wird der Vaterknoten des Startknotens auf sich selbst gesetzt. In Zeile 7 wird die Warteschlange initialisiert und bekommt als erstes Element den Startknoten. Nun wird die \lstinline{$\textbf{while}$} Schleife von Zeile 8-14 solange ausgeführt, solange es Knoten gibt, die noch nicht besucht wurden, aber vom Startknoten erreichbar sind. Innerhalb der \lstinline{$\textbf{while}$} Schleife wird in Zeile 9 das Element nach dem FIFO Prinzip aus der Warteschlange genommen, das heißt das Element, das zeitlich am längsten in der Warteschlange war. Die \lstinline{$\textbf{for}$} Schleife in den Zeilen 10-14 iteriert über die Nachbarn \lstinline{v $\in$ G.Adj[u]} des aktuellen Knoten \lstinline{u}. Als Nachbar werden folgende Knoten bezeichnet, die adjazent zum jeweiligen Knoten sind, also durch eine Kante verbunden sind. In Zeile 11 wird überprüft, ob der Knoten \lstinline{v} noch nicht besucht wurde, was mittels \lstinline{$\textbf{if}$} Bedingung
\lstinline{v.d == $\infty$} geprüft wird. Ist dies der Fall, wird \lstinline{v.d} auf \lstinline{u.d + 1} gesetzt und \lstinline{u} wird als Vater in \lstinline{v.$\pi$} gekennzeichnet. Danach wird der Knoten \lstinline{u} in die Warteschlange aufgenommen. Der Algorithmus endet, wenn bei der Überprüfung der Bedingung in Zeile 8 keine Elemente in der Warteschlange vorhanden sind, was bedeutet, dass alle erreichbaren Knoten besucht wurden und die Schleife abbricht.\\
Dadurch dass jedem besuchten Knoten \lstinline{u} mit \lstinline{u.$\pi$} der Vaterknoten zugewiesen wird, entsteht durch die Breitensuche ein Subgraph von \(G\) der Vaterknoten, definiert durch \(G_{\pi} = (V_{\pi}, E_{\pi})\), wobei \(V_{\pi} = \{v \in V : v.\pi \neq\) \lstinline{NIL}\(\}\) und \(E_{\pi} = \{\{v.\pi,v\} : v \in V_{\pi} - \{v_{0}\}\}\). Beim Subgraph \(G_{\pi}\) handelt es sich um einen Baum (\textbf{\textit{breadth-first tree}}), der alle Knoten \(V_{\pi} \) enthält, die vom Startknoten \(v_{0}\) erreichbar sind. Da es sich um einen Baum handelt, existiert für alle Knoten \(v \in V_{\pi}\) im Subgraphen \(G_{\pi}\) genau ein Pfad von \(v_{0}\) nach \(v\), der gleichzeitig auch der kürzeste Pfad von \(v_{0}\) nach \(v\) in \(G\) ist.
\subsection{Analyse von Algorithmen}
Um Vorherzusagen, wie viele Ressourcen ein Algorithmus benötigen wird, ist die Analyse eines Algorithmus entscheidend. Dabei ist für uns die Ressource \textit{Laufzeit} maßgebend. Falls man mehrere Algorithmen zur Lösung eines Problems zur Verfügung hat, wird man sich meist für den Algorithmus mit der besten Laufzeit entscheiden.\\
Um einen Algorithmus maschinenunabhängig analysieren zu können, macht man Gebrauch vom RAM (\textit{random-access machine}) Modell. Dieses abstrakte Rechnermodell hat einen einzigen Prozessor und Instruktionenen werden sequentiell ausgeführt. Folgende Annahmen werden getroffen:
\begin{itemize}
	\item{Eine einfache Operation wird in einem Schritt ausgeführt.}
	\item{Subroutinen und Schleifen sind keine einfachen Operationen.}
	\item{Der Speicher ist unbegrenzt, ein Speicherzugriff benötigt einen Schritt.}
\end{itemize}
Die Laufzeit des Algorithmus ist nun die Anzahl der benötigten Schritte, die abhängig von der Länge \(n\) der Eingabe ist und für größer werdende \(n\) asymptotisch unter Verwendung der Landau-Notation abgeschätzt wird. Eine ausführliche und weiterführende Beschreibung der Analyse von Algorithmen ist in \cite{cormen_introduction_2009} von Cormen et al. zu finden.\\
Um die sequentielle Breitensuche im RAM Modell zu analysieren, ist es nun entscheidend von welchen Eingaben die Laufzeit abhängt. Es stellt sich heraus, dass die Anzahl der Knoten \(n = |V(G)|\) und der Kanten \(m = |E(G)|\) eines Graphen die Laufzeit der Breitensuche beeinflussen. Algorithmus~\ref{fifo} weist eine Laufzeit von \($O$(n + m)\) auf, da im schlimmsten Fall alle Knoten und damit auch alle Kanten besucht werden. Die Anzahl der Iterationen, Zeile 8-14, ist dabei gebunden an die Anzahl der Knoten \(|V(G)|\) und innerhalb jedes Knoten werden all seine Kanten, Zeile 10-14, angesehen. 
\section{Parallele Breitensuche}
\label{parallel}
In der Praxis ist es oft nicht ausreichend eine sequentielle Variante der Breitensuche zur Verfügung zu haben, da es sich oft um sogenannte "'big data"' Anwendungsbereiche handelt, also um Anwendungsbereiche die eine große Datenmenge zu Grunde liegen haben. Die sequentielle Variante, die auf einem einzigen Prozessor ausgeführt wird, ist einfach zu langsam für größer werdende Graphen. Daher ist es unser Ziel, einen parallelen Algorithmus für die Breitensuche zu finden, der den Graphen parallel traversiert und im Vergleich zur sequentiellen Variante einen Speedup erzielt. Der \textit{Speedup}-Begriff, wie durch Rauber und Rünger~\cite{rauber} definiert, wird als Maß für den relativen Geschwindikeitsgewinn herangezogen. Der Speedup \(S_{p}(n)\) ist definiert als
\begin{equation}
	S_{p}(n) = \frac{T^{*}(n)}{T_{p}(n)}
\end{equation}
Dabei ist \(T^{*}(n)\) die Laufzeit einer optimalen sequentiellen Implementierung und \(T_{p}(n) \) die Laufzeit eines parallelen Programmes. \(p\) steht für die Anzahl der zur Verfügung stehenden Prozessoren zur Lösung eines Problems der Größe \(n\). Es gilt \(S_{p}(n) \leq p\). Wäre \(S_{p}(n) \geq p\), könnte man einen sequentiellen Algorthmus finden, der schneller als der für die Berechnung des Speedups verwendete ist.\\
Neben der Laufzeit und des Speedups eines parallelen Programms spielt in unserer Analyse von parallelen Lösungen noch ein dritter Faktor eine wichtige Rolle, nämlich die Kosten. Die \textit{Kosten} eines parallelen Programms sind nach Rauber und Rünger \cite{rauber} definiert als
\begin{equation}
	C_{p}(n) = T_{p}(n) \cdot p
 \end{equation}
und ist die von allen Prozessoren ausgeführte Arbeit. Unser Ziel in Bezug auf die Kosten ist es, ein paralleles Programm zu finden, dass \textit{kostenoptimal} beziehungsweise annähernd kostenoptimal ist, also bei dem gilt \(C_{p}(n) = T^{*}(n)\). Das heißt es sollen vom parallelen Programm insgesamt genauso viele Operationen ausgeführt werden wie von einem optimalen sequentiellen Verfahren.
\subsection{Erste Überlegungen}
Nachdem beschrieben wurde, wie ein paralleles Programm in unserer Arbeit bewertet wird, auch in Bezug auf das sequentielle Verfahren, wenden wir uns wieder dem eigentlichen Thema zu, der Breitensuche und deren Parallelisierung. Es ist bei vielen Problemen schwierig einen sequentiellen Algorithmus mit nur wenigen Änderungen in einen parallelen umzusetzen. Oft muss man eine andere Sicht auf das zu lösende Problem bekommen und eine ähnliche oder andere Lösung heranziehen, bei der eine parallele Umsetzung besser möglich ist. Ein Beispiel dafür ist Algorithmus~\ref{fifo}.  Dieser Algorithmus zur Lösung der Breitensuche ist schwierig zu parallelisieren, da die FIFO Warteschlange ein Hindernis für die Parallelisierung auf Rechnern mit physikalisch verteiltem Speicher darstellt, falls man diese im parallelen Algorithmus synchronisiert halten will. Ein ausführlichere Beschreibung liefert hierbei Leiserson und Schardl~\cite{leiserson}.\\
Aus diesem Grund geben wir noch einen weiteren sequentiellen Algorithmus durch Algorithmus~\ref{stacks} an, der anstatt der FIFO Warteschlange zwei Stapelspeicher (\textit{stacks}) verwendet, wodurch ein level-basiertes Durchsuchen des Graphen, wie in Abbildung~\ref{fig:graph_level}, ersichtlicher wird. Mit Hilfe von Algorithmus~\ref{stacks} ist es einfacher, einen parallelen Algorithmus abzuleiten. Dieser Algorithmus wird auch von Buluc und Madduri~\cite{buluc} als Basis für deren Arbeit zur parallelen Breitensuche verwendet und ist auch in deren Arbeit abgebildet.
\begin{lstlisting}[caption={Eine weitere sequentielle Variante der Breitensuche unter Verwendung von zwei Stacks \lstinline{FS} und \lstinline{NS} als Datenstrukturen. Dieser Algorithmus liefert das gleiche Ergebnis wie Algorithmus~\ref{fifo} und hat auch die gleiche Laufzeit wie dieser, ermöglicht jedoch eine bessere Sicht auf das level-basierte Durchsuchen des Graphen, was das Ableiten eines parallelen Algorithmus einfacher macht.},label=stacks]
$\textbf{SERIAL-BFS-STACKS}$(G,$s$)
	$\textbf{for}$ each vertex u $\in$ V(G) - {$s$}
		d[u] = $\infty$
		$\pi$[u] = NIL
	d[s] = 0
	$\pi$[s] = s
	level = 1
	FS = {s}
	NS = $\emptyset$
	$\textbf{while}$ FS $\neq$ 0
		$\textbf{for}$ each u $\in$ FS
			$\textbf{for}$ each v $\in$ G.Adj[u]
				$\textbf{if}$ d[v] == $\infty$
					d[v] = level
					$\pi$[v] = u
					NS = NS $\cup$ {v}
		FS = NS
		NS = $\emptyset$
		level = level + 1
\end{lstlisting}
Algorithmus~\ref{stacks} ist sehr ähnlich zu Algorithmus~\ref{fifo}. Es werden jedoch zwei Datenstrukturen zur Durchsuchung des Graphen benötigt und es gibt eine weitere verschachtelte Schleife in Zeile 11. Bei den zwei Datenstrukturen handelt es sich um \lstinline{FS} (\textit{frontier stack}), der Stapelspeicher, wo die Knoten des aktuellen Levels gespeichert sind, und \lstinline{NS} (\textit{newly-visited stack}), der Stapelspeicher, wo die neu besuchten Knoten gespeichert werden, also die Knoten, die ein um eins höheres Level haben als die Knoten in \lstinline{FS}. Einen Unterschied gibt es auch in der Abspeicherung der Distanz \(d\) und des Vaterknotens \(\pi\) zu jedem Knoten. Im Gegensatz zu Algorithmus~\ref{fifo} werden diese Informationen nicht direkt beim Knoten über ein Attribut abgespeichert, sondern in einer eigenen Datenstruktur \lstinline{d[1..n]} und \lstinline{$\pi$[1..n]}. In diesem Fall handelt es sich um Arrays der Länge \lstinline{n}, wobei es sich bei \lstinline{n} um die Anzahl der Knoten handelt.\\
Nach der Initialisierung wird die \lstinline{$\textbf{while}$} von Zeile 10-19 solange ausgeführt bis es keine neu besuchten Knoten gibt, also keine weiteren Knoten ausgehend vom Startknoten \lstinline{$v_{0}$} erreicht werden können. In den Zeilen 11-16 wird mittels \lstinline{$\textbf{for}$} Schleife über alle Knoten des aktuellen Levels iteriert. Durch die Bedingung in Zeile 10 muss zumindest ein Element \lstinline{u} in \lstinline{FS} enthalten sein. Über die Nachbarn des Knoten \lstinline{u} in Zeile 12-16 wird genauso iteriert wie in Algorithmus~\ref{fifo}, außer dass hier die Nachbarn \lstinline{v}, die zuvor noch nicht besucht wurden, also \lstinline{d[v] == $\infty$}, im Stapelspeicher \lstinline{NS} abgelegt werden. Ebenfalls sehr entscheidend für diesen Algorithmus ist, dass nachdem alle Knoten des aktuellen Levels abgearbeitet wurden, der Stapelspeicher \lstinline{FS} die Elemente von \lstinline{NS} übernimmt und \lstinline{NS} geleert wird. \lstinline{FS} hat also alle Knoten nun gespeichert, die zuvor in Zeile 14-16 neu besucht wurden. Danach wird der Wert von \lstinline{level}, der den neu besuchten Knoten durch \lstinline{d[v] = level} in Zeile 14 zugewiesen wird, um eins erhöht und die nächste Iteration mit den neu besuchten Knoten startet.\\
Als Ausgabe erhalten wir genauso wie bei Algorithmus~\ref{fifo} einen \textit{breadth-first tree}. Die Laufzeit von Algorithmus~\ref{stacks} ist genauso wie bei Algorithmus~\ref{fifo} \($O$(n + m)\), wobei die Anzahl der Iterationen von der \lstinline{$\textbf{while}$} Schleife von Zeile 10-19 gebunden ist an die Länge des längsten Pfades vom Startknoten \(s\) zu einem erreichbaren Knoten im Graphen \(G\), was dem Durchmesser (\textit{diameter}) \(D\) des Subgraphen \(G_{\pi}\) entspricht.\\
Anhand dieser Überlegungen gibt es nun unterschiedliche Wege die Breitensuche zu parallelisieren. Wie schon durch Buluc und Madduri~\cite{buluc} spezifiert, gibt es zwei grundlegende Varianten, wie man dieses Problem parallelisiert, nämlich durch Zerlegung beziehungsweise Aufteilung des Graphen auf die zur Verfügung stehenden Prozessoren. Natürlich hängt diese Zerlegung auch von der Repräsentation des Graphen in der Implementierung ab, womit wir uns aber erst in Kapitel~\ref{sec:details} genauer beschäftigen werden. Vorerst ist nur die Idee der Zerlegung wichtig. Wir gehen davon aus, dass die Knoten des Graphen durchgehend nummeriert sind und dass wir mit jeder Knotennummer direkt auf die Kanten dieses Knoten zugreifen können.
\begin{itemize}
	\item{Eindimensionale (\textbf{1D}) Zerlegung des Graphen bedeutet nun, dass man die Knoten des Graphen \(V(G)\) auf die Prozessoren so aufteilt, dass wenn \(n = |V(G)|\) jeder Prozessor in Besitz von \(n / p\) Knoten ist, wobei \(p\) die Anzahl der verfügbaren Prozessoren darstellt. Zusätzlich besitzt jeder Prozessor lokal die Kanten, die mit einem auf dem Prozessor gespeicherten Knoten verbunden sind, also jede Kante \(e = \{v_{1},v_{2}\}, e \in E(G)\), wo entweder \(v_{1}\) oder \(v_{2}\) ein lokaler Knoten des Prozessors ist, da es sich um einen ungerichteten Graphen handelt.}
	\item{Bei der zweidimensionalen (\textbf{2D})  Zerlegung werden nicht nur die Knoten unter den Prozessoren aufgeteilt, sondern es werden auch die zu den Knoten zugehörigen Kanten unter mehreren Prozessoren aufgeteilt. Das heißt es kann sein, dass mehrere Prozessoren die gleiche Knotenmenge besitzen, jedoch unterschiedliche Kanten zu diesen Knoten.}
\end{itemize}
Diese Zerlegungen lassen sich sehr gut mit einer Adjazenzmatrix darstellen, es ist jedoch nicht notwendig auch eine Matrix als Graphrepräsentation in der Implementierung zu verwenden, diese Repräsentation soll nur zum Verständnis beitragen.
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=0.9\textwidth]{eindim}
        \caption{eindimensional}
        \label{fig:eindim}
    \end{subfigure}
   \qquad
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=0.9\textwidth]{zweidim}
        \caption{zweidimensional}
        \label{fig:zweidim}
    \end{subfigure}
    \caption{Es ist möglich, den Graphen ein- oder zweidimensional zu zerlegen. Diese abstrakte Darstellung einer Adjazenzmatrix in Abbildung~\ref{fig:eindim} zeigt, dass im eindimensionalen Fall die Zerlegung nur auf einer Achse erfolgt, im zweidimensionalen Fall in Abbildung~\ref{fig:zweidim} auf zwei Achsen.}
\label{fig:zerlegung}
\end{figure}
Abbildung~\ref{fig:zerlegung} zeigt die Zerlegung des Graphen im ein- und zweidimensionalen Fall. Repräsentiert wird der Graph durch eine Adjazenzmatrix. Das heißt, dass auf beiden Achsen die Knoten eingetragen sind und jeder Eintrag in der Matrix die Information speichert, ob eine Kante zwischen diesen Knoten existiert oder nicht.  Im eindimensionalen Fall heißt das, dass die Knoten, die auf der horizontalen Achse eingetragen sind, und alle zugehörigen Kanten, dargestellt durch die Spalten in der Matrix, unter den Prozessoren aufgeteilt werden. Ein Rahmen (\textit{frame}) spiegelt also wider, was in einem Prozessor abgespeichert wird. Im Gegensatz dazu wird bei der zweidimensionalen Zerlegung auch die vertikale Achse auf mehrere Prozessoren aufgeteilt, wodurch es, wie schon weiter oben erwähnt, der Fall ist, dass sich mehrere Prozessoren die gleiche Knotenmenge teilen, aber unterschiedliche Kanten zu diesen Knoten abspeichern beziehungsweise behandeln.\\
Ausgehend von der level-basierten sequentiellen Breitensuche, dargestellt durch Algorithmus~\ref{stacks}, und der eindimensionalen Zerlegung des Graphen, werden wir zwei Algorithmen zur parallelen Breitensuche abstrakt veranschaulichen. Grundlegender Unterschied dieser zwei Algorithmen ist die Synchronisation zwischen den Prozessoren, also welche Daten werden wie synchronisiert. Außerdem liegen Unterschiede darin, wieviel die Prozessoren über den Gesamtzustand des Systems wissen, also zum Beispiel ob Knoten, die anderen Prozessoren zugeordnet sind, bereits besucht wurden und deshalb im nächsten Level nicht mehr berücksichtigt werden müssen. Wichtig ist jedoch, dass beide Algorithmen mit einer 1D Zerlegung des Graphen arbeiten und als Ausgabe einen \textit{breath-first tree} haben müssen. Falls dieser \textit{breath-first tree} in einem Array abgespeichert ist, bezeichnen wir dieses Array auch als \textit{Parent Array}.  Dabei wollen wir festlegen, dass dieses \textit{Parent Array} am Ende des Algorithmus nicht unbedingt auf einem Prozessor liegen muss, sondern auf den arbeitenden Prozessoren verteilt sein kann. Details über die Implementierung und welche Auswirkungen diese Unterschiede auf die Laufzeit des Algorithmus haben, sind in Kapitel~\ref{sec:details} beziehungsweise Kapitel~\ref{sec:analyse} zu finden.
\subsection{Paralleler BFS mit Allreduce Kommunikation}
\label{sec:bfs_allreduce}
Die Idee hinter diesem Algorithmus war neben der eindimensionalen Zerlegung des Graphen und damit parallelen Abarbeitung der Levels, dass die Prozessoren mehr Informationen über die anderen Prozessoren haben und deren Knoten, was in diesem Algorithmus bei der Erstellung des \textit{Parent Array} zugute kommt. Bei vielen parallelen Algorithmen der Breitensuche, wie auch beim Algorithmus in Kapitel~\ref{sec:bfs_alltoall} ist es so, dass die Prozessoren nur Informationen über die lokal gespeicherten Knoten besitzen und anhand dieser Informationen Entscheidungen treffen müssen, wodurch es zu einen erhöhten Kommunikationsaufwand zwischen den Prozessoren kommen kann. Zum Beispiel ist es einem Prozessor, der die Nachbarn eines neu besuchten Knoten durchläuft, bei diesen Algorithmen nicht bekannt, ob ein Knoten, der nicht zu den lokalen gehört, bereits besucht wurde, da dies nur der Prozessor weiß, dem der Knoten zugeteilt ist. Deshalb muss jedes Mal bei einem für einen Prozessor fremden Knoten, zwischen den Prozessoren kommuniziert werden, ob dieser Knoten bereits in einem vorherigen Level besucht wurde.\\
Aufgrund dieser Tatsache wird bei unserem ersten Algorithmus eine Datenstruktur zwischen den Prozessoren synchron gehalten, die über alle Knoten des Graphen die Information besitzt, ob ein Knoten besucht wurde oder nicht. Falls wir uns also gerade im Level \(d\) befinden, weiß jeder Prozessor welche Knoten im Graphen bis einschließlich zum Level \(d-1\) besucht wurden. Dazu verwenden wir als Datenstruktur ein Array \lstinline{a[1..n]}, wie in Algorithmus~\ref{par_allvisited} ersichtlich, das zu jedem Knoten die Information speichert, ob dieser Knoten bisher besucht wurde oder nicht. Dies stellen wir durch Abspeichern der Werte 0 (nicht besucht) und 1 (besucht) in \lstinline{a[k]} zu jedem Knoten \(k \in V(G)\) sicher. Da wir anhand dieser Datenstruktur nur wissen, welche Knoten bis einschließlich des letzten Levels besucht wurden, jedoch nicht, welche davon noch nicht von den zuständigen Prozessoren bearbeitet wurden (also im letzten Level neu besucht wurden), braucht jeder Prozessor noch ein zweites Array \lstinline{v[1..n/p]}, welches zu jedem lokalen Knoten abspeichert, 0 oder 1, ob dieser Knoten lokal behandelt wurde und damit auch die Nachbarn dieses Knoten besucht wurden. Dieses Array hat die Länge \lstinline{n/p}, da jeder Prozessor für \lstinline{n/p} Knoten verantwortlich ist. Dabei ist \lstinline{p} die Anzahl der zur Verfügung stehenden Prozessoren. Zu beachten ist auch, dass alle Prozessoren ein \textit{Parent Array} \lstinline{$\pi$} der Länge \(n\) besitzen. Falls also ein Knoten \(x\) ausgehend von Knoten \(y\) zum ersten Mal besucht wird, dann trägt der Prozessor, der für \(y\) verantwortlich ist, den Knoten \(y\) als Vater von \(x\) ein, also \lstinline{$\pi$[x] = y}, wobei es sich bei \lstinline{$\pi$}, um das auf dem Prozessor lokal gespeicherte \textit{Parent Array} handelt.
\begin{lstlisting}[caption={Die Prozedur \lstinline{PARALLEL-BFS-ALLREDUCE} wird von allen verfügbaren Prozessoren parallel aufgerufen. Nach jeder Abarbeitung eines Levels findet durch die \lstinline{ALLREDUCE} Operationen eine sogenannte Barrier-Synchronisation statt, bei der zwischen den Prozessoren ausgetauscht wird, ob es überhaupt einen neu besuchten Knoten gibt (\lstinline{isNewLevel}), beziehungsweise wird das Array \lstinline{a} synchronisiert.},label=par_allvisited]
$\textbf{PARALLEL-BFS-ALLREDUCE}$($G_{p}$,s)
	$\textbf{for}$ 0 $\leq$ i $<$ n $\textbf{do}$
		a[i] = 0
		$\pi$[i] = NIL
	$\textbf{for}$ 0 $\leq$ i $<$ n/p $\textbf{do}$
		v[i] = 0
	a[s] = 1
	isNewLevel = 1
	$\pi$[s] = s
	$\textbf{while}$ (isNewLevel == 1)
		isNewLevel = 0
		$a_{p}$ = a[n/$\textit{p}$*$\textit{rank}$+1..n/$\textit{p}$*($\textit{rank}$+1)]
		FS = $\textit{GETNEWVISITED}$($a_{p}$,v)
		$\textbf{for}$ each u $\in$ FS
			v[u] = 1
			$\textbf{for}$ each v $\in$ $G_{p}$.Adj[u]
				$\textbf{if}$ a[v] == 0
					a[v] = 1
					$\pi$[v] = u
					isNewLevel = 1
		ALLREDUCE(isNewLevel,OR)
		ALLREDUCE(a,OR)
	REDUCE($\pi$, MAX, $p_{0}$)
\end{lstlisting}
Um den Algorithmus besser zu strukturieren, verwenden wir das \textbf{BSP-Modell} (\textit{bulk synchronous parallel}), das eine Abstraktion eines Rechners mit physikalisch verteiltem Speicher darstellt, so wie von Rauber und Rünger in \cite{rauber} beschrieben. Grundlegend für dieses Modell, wie in Abbildung~\ref{fig:bsp} ersichtlich, ist, dass eine Berechnung (Ergebnis eines Algorithmus) aus einer Folge von \textbf{Superschritten} besteht. In jedem Superschritt führt jeder Prozessor lokale Berechnungen durch und am Ende findet eine Barrier-Synchronisation statt, die durch globale Kommunikationoperationen bewerkstelligt wird. Wichtig in diesem Modell ist jedoch, dass die Barrier-Synchronisation für alle Prozessoren zum gleichen Zeitpunkt stattfindet und dass die verschickten Daten erst im nächsten Superschritt verwendet werden.
\begin{figure}[h]
 	\centering
	\includegraphics[width=0.5\textwidth]{bsp}
 	\caption{Beim BSP-Modell entsteht ein Resultat eines Algorithmus aus einer Vielzahl von sogenannten Superschritten. Man kann sagen, dass ein Superschritt wiederum aus mehreren Schritten besteht, nämlich aus den lokalen Berechnungen der beteiligten Prozessoren, der globalen Kommunikationsoperationen zwischen den Prozessoren beziehungsweise dem Austausch von Daten und der Barrier-Synchronisation, die durch diese Kommunikationsoperationen bewerkstelligt wird. Wichtig für dieses Modell ist, dass die Prozessoren jene Daten in den lokalen Berechnungen verwenden, die sie aus der Kommunikation des letzten Superschritts erhalten haben.}
	\label{fig:bsp}
\end{figure}
In Algorithmus~\ref{par_allvisited} wird in einem Superschritt ein Level bearbeitet und entspricht einer Iteration der \lstinline{$\textbf{while}$} Schleife von Zeile 10-22.  Jeder Prozessor führt innerhalb dieses Superschrittes seine lokalen Berechnungen durch, bis am Ende die Barrier-Synchronisation durch die \lstinline{ALLREDUCE} Operationen durchgeführt wird.\\
Jeder Prozessor ruft die Prozedur \lstinline{PARALLEL-BFS-ALLREDUCE} auf und hat als Eingabe einen Teilgraphen \lstinline{$G_{p}$} und den Startknoten \lstinline{s}. Beim Teilgraphen \lstinline{$G_{p}$} handelt es sich um den eindimensional zerlegten Graphen. Das heißt jeder Prozessor besitzt \(n/p\) Knoten, welche durch eine Identifikationsnummer eindeutig gekennzeichnet sind, und alle Kanten zu den zugehörigen \(n/p\) Knoten. Dabei können wir davon ausgehen, das der erste Prozessor die ersten \(n/p\) Knoten zugeteilt bekommt, der zweite die zweiten \(n/p\) Knoten und so weiter. Zu Beginn findet wie bei den vorherigen Algorithmen eine Initialisierung der Datenstrukturen und Variablen statt. Der Startknoten \lstinline{s} wird im Array \lstinline{a} gesetzt. Außerdem brauchen wir eine zusätzliche Variable \lstinline{isNewLevel}, die die Information speichert, ob im letzten Superschritt zumindest ein Knoten besucht wurde. Diese Variable ist notwendig, da die Prozessoren anhand der Informationen der Arrays \lstinline{a} und \lstinline{v} nur herausfinden können, ob ein lokaler Knoten neu besucht wurde, nicht aber ob ein Knoten eines anderen Prozessors neu besucht wurde. Diese Variable wird am Ende jedes Superschrittes synchronisiert und nimmt den Wert 0 in Zeile 21 an, falls kein neuer Knoten im letzten Superschritt hinzugekommen ist. In Zeile 12 wird durch \lstinline{a[n/$\textit{p}$*$\textit{rank}$+1..n/$\textit{p}$*($\textit{rank}$+1)]} der Teil vom Array \lstinline{a} herausgenommen, der die Knoten enthält, für die der jeweilige aufrufende Prozessor zuständig ist. Mithilfe dieses Ausschnitts \lstinline{$a_{p}$} und des Arrays \lstinline{v}, die jeweils eine Länge von \(n/p\) haben, können nun mit der Funktion \lstinline{GETNEWVISITED} die Knoten herausgefiltert werden, die im letzten Superschritt neu besucht worden sind. Die Abarbeitung der zu bearbeitenden Knoten ist sehr ähnlich zu Algorithmus~\ref{stacks}, außer dass zusätzlich die Variable \lstinline{isNewLevel} auf 1 gesetzt wird, sobald ein neuer Knoten besucht wird.\\
Am Ende des Algorithmus findet die Barrier-Synchonisation statt, die mittels \lstinline{ALLREDUCE} Operationen ausgeführt wird. Um die \lstinline{ALLREDUCE} Operation zu erklären, werden wir zuerst auf die Operation \lstinline{REDUCE} eingehen, da man sagen kann, dass \lstinline{ALLREDUCE} in gewissen Sinne eine Erweiterung dieser Operation ist. Bei \lstinline{REDUCE} handelt es sich um eine Akkumulationsoperation oder auch globale Reduktionsoperation, bei der jeder beteiltigte Prozessor Daten zur Verfügung stellt, die mittels einer binären Operation verknüpft werden. Das Ergebnis erhält dann ein definierter Wurzelprozessor. \lstinline{ALLREDUCE} ist eine Multi-Akkumulationsoperation, bei der man sagen kann, dass jeder beteiligte Prozessor eine Einzelakkumulationsoperation ausführt. Einfach ausgedrückt stellt also jeder Prozesso ein Datum zur Verfügung, das heißt es gibt bei \(p\) Prozessoren \(p\) Daten. Diese Daten werden dann mit einer definierten binären Operation zu einem Datum verknüpft und an alle Prozessoren gesendet. Es erhalten somit alle Prozessoren das selbe Ergebnis.\\
Für die Barrier-Synchronisation in Algorithmus~\ref{par_allvisited} wird sowohl \lstinline{isNewLevel} als auch das Array \lstinline{a} synchronisiert. Es werden bei der \lstinline{ALLREDUCE} Operation von jedem Prozessor die Daten \lstinline{isNewLevel} beziehungsweise \lstinline{a} herangenommen und zu einem einzigen Datum verknüpft. Damit keine Information verloren geht, also eine gesetzte 1 entweder in \lstinline{isNewLevel} oder an einer bestimmten Position von \lstinline{a} auch durch die \lstinline{ALLREDUCE} Operation im Ergebnis gesetzt bleibt, kommt nur eine binäre Operation für die Verknüpfung in Frage, nämlich die Operation \lstinline{OR}.\\
Die Schleife von Zeile 10-22 bricht an dem Punkt ab, bei dem kein einziger neuer Knoten von einem Prozessor besucht wird, also die Variable \lstinline{isNewLevel} den Wert 0 annimmt. Da nun ein \textit{Parent Array} \lstinline{$\pi$} als Ergebnis vorhanden ist, dieses jedoch noch auf den einzelnen Prozessoren verteilt ist oder anders gesagt jeder Prozessor nur einen Teil des \textit{breadth-first trees} besitzt, wird dieses per \lstinline{REDUCE} Operation verknüpft und das Ergebnis an den Wurzelprozessor \lstinline{$p_{0}$} geliefert. Als binäre Verknüpfungsoperation haben wir \lstinline{MAX} gewählt, da es sein, dass es mehrere Väter zu einem bestimmten Knoten gibt und der Knoten gewählt wird, der die höhere Identifikationsnummer besitzt. Dieser Fall kann auftreten, wenn ein bestimmter Knoten im gleichen Level von mehrereren Vaterknoten zum ersten Mal besucht wird und diese Vaterknoten, wenn diese zu unterschiedlichen Prozessoren gehören, nicht wissen, dass dieser Knoten gleichzeitig auch von Knoten anderen Prozessoren besucht wird. Dadurch können sich mehrere Knoten als Vater zu denselben Knoten in unterschiedliche lokale Versionen von \lstinline{$\pi$} eintragen.
\begin{table}[h]
\centering
\label{tbl:allreduce}
\begin{tabular}{lcl}
$P_{0}$ : $x_{0}$     		& \multirow{4}{*}{$\Rightarrow$} 	& $P_{0}$ : $x_{0}$ + $x_{1}$ + ... + $x_{p-1}$ \\
$P_{1}$ : $x_{1}$     		&                                 						& $P_{1}$ : $x_{0}$ + $x_{1}$ + ... + $x_{p-1}$ \\
...         			&                                 									& ... \\
$P_{p-1}$ : $x_{p-1}$ 	&                                 						& $P_{p-1}$ : $x_{0}$ + $x_{1}$ + ... + $x_{p-1}$
\end{tabular}
\caption{Die Tabelle, wie sie auch von Rauber und Rünger~\cite{rauber} verwendet wird, zeigt das Resultat der \lstinline{ALLREDUCE} Operation. Jeder Prozessor stellt jeweils ein Datum $x$ zur Verfügung. Diese Daten werden per \lstinline{ALLREDUCE} Operation oder genauer gesagt anhand der binären Operation $+$ zu einem Datum verknüpft und an die beteiligten Prozessoren gesendet.}
\end{table}
\subsection{Paralleler BFS mit All-to-all Kommunikation}
\label{sec:bfs_alltoall}
\section{Implementierungsdetails}
\label{sec:details}
Bevor die Algorithmen analysiert und die dazugehörigen Versuchsergebnisse präsentiert werden, werden wir auf gewisse Details der Implementierung der Algorithmen eingehen und erklären, warum wir uns für gewisse Vorgehensweisen und Datenstrukturen entschieden haben. Die verwendete Programmiersprache ist C, für die Kommunikation zwischen der Knoten der DMM (\textit{distributed memory machine}) haben wir MPI und für die Kommunikation innerhalb eines Knoten, im Fall der Implementierung des Hybrid-Variante, OpenMP verwendet. Das Message-Passing-Programmiermodell hat selbst keine explizite Sicht auf die Topologie eines Rechners mit physikalisch verteiltem Speicher und arbeitet stets mit einer bestimmten Anzahl von Prozessen mit zugeordneten Daten, was zu einer Abstraktion einer DMM führt. Dies führt zu einer Portabilität der Programme, da man keine genauen Informationen über die Rechner kennen muss.\\
Diese Prozesse werden wir in Folge als MPI Prozesse bezeichnen. Dabei können wir bei der Ausführung der Programme sehr wohl steuern, wie viele Knoten einer DMM beziehungsweise wie viele Prozessoren eines Knoten verwendet werden.
\subsection{Graphrepräsentation}
\label{sec:graphrepresentation}
Es gibt viele Möglichkeiten einen Graph abzuspeichern. Ein Beispiel dafür wäre die gesamte Adjazenzmatrix im Speicher abzulegen. Bei der Adjazenzmatrix handelt es sich um ein zweidimensionales Array, das zu jedem möglichen Knotenpaar die Information vorliegen hat, ob eine Kante zwischen diesen beiden Knoten existiert oder nicht. Da für unsere Versuche und Anwendungsbereiche jedoch nur \textbf{dünne Graphen} verwendet werden, also Graphen bei denen die Kantenanzahl ein konstanter Faktor der Knotenanzahl  darstellt, wäre diese Weise der Abspeicherung speicherineffizient, da hauptsächlich Speicher für die Information benötigt wird, dass zwischen beliebigen Knoten keine Kante vorhanden ist. Aus diesem Grund haben wir uns für eine Speichermethode entschieden, bei der nur existierende Kanten abgespeichert werden.\\
Diese Methode ist ähnlich zur CSR (\textit{compressed sparse row})-Repräsentation einer Matrix, wie sie auch von Buluc und Madduri~\cite{buluc} verwendet wird. Da wir jedoch keine speziellen Informationen bezüglich Spalte und Zeile einer Kante benötigten, bedienen wir uns einer etwas einfacheren Methode. Wir legen die Kanten in einem Array der Länge \(m\) ab, wobei \(m = |E(G)|\). Abgespeichert werden dazu die Kanten von Knoten \(v+1\) neben den Kanten von Knoten \(v\). Im Array werden die Identifikationsnummern der Knoten zu denen die Kante führt abgespeichert. Da wir mit ungerichteten Graphen arbeiten, wird jede Kante zweimal abgespeichert, jeweils einmal für den Knoten der mit der jeweiligen Kante verbunden ist. Außerdem legen wir fest, dass wir 64-Bit Integer für die Identifikation von Knoten verwenden.\\
Damit ersichtlich ist, wo die Kanten eines bestimmten Knoten beginnen, ist noch ein weiteres Array der Länge \(n+1\) notwendig, dabei ist \(n = |V(G)|\). Jeder Feld dieses Hilfsarrays gibt dabei die Position des Knoten im Kantenarray zurück. Abbildung~\ref{fig:graphrepresentation} zeigt diese Graphrepräsentation in einem Beispiel. Zu sehen ist zu den Knoten und Kanten des Graphen das Array, das die Kanten speichert, und darunter das Hilfsarray, das zu jedem Knoten die Position des Kantenarrays ablegt. Beim Hilfsarray benötigt man deswegen \(n+1\) Felder, um zu wissen, wie viele Kanten der letzte Knoten besitzt.
\begin{figure}[h]
 	\centering
	\includegraphics[width=0.9\textwidth]{graphrepresentation}
 	\caption{Zum Graphen werden die Kanten im oberen Array abgespeichert, sodass die Kanten von Knoten \(v+1\) neben den Kanten von Knoten \(v\) liegen. Um feststellen zu können, wo die Kanten des jeweiligen Knoten im Array beginnen, wird noch ein Hilfsarray benötigt, das zu jedem Knoten die Position abspeichert. Schlingen im Graphen werden nicht abgespeichert.}
	\label{fig:graphrepresentation}
\end{figure}
\subsection{Paralleler BFS mit Allreduce Kommunikation}
\label{sec:allreduce_details}
Wichtig für die Implementierung dieses Algorithmus ist, dass wir für das Array \lstinline{a[1..n]} und \lstinline{v[1..n/p]} aus Algorithmus~\ref{par_allvisited} eine sogenannte \textbf{\textit{Bitmap}} beziehungsweise ein Bit Array verwenden. Da bei diesen Arrays nur zwischen der Information 0 oder 1 unterschieden werden muss, ist hier ein Bit pro Knoten für diese Information ausreichend. Es wäre in Bezug auf den Speicher platzverschwendend, wenn man pro Knoten einen Speicher von mehreren Byte allokiert, nur damit man die Zahl 0 oder 1 abspeichert.\\
Deshalb wird bei einer \textit{Bitmap} wirklich jedes Bit eines Wertes vom Typ Integer ausgenutzt. In unserer Implementierung verwenden wir vorzeichenlose Integer mit einer Größe von 64 Bits. Damit können wir für jeden dieser Integer-Werte, in Bezug auf die Information 0 oder 1, genau 64 Knoten abdecken.\\
Ebenfalls maßgebend ist die Implementierung der Funktion \lstinline{GETNEWVISITED} aus Algorithmus~\ref{par_allvisited}, da diese eine wichtige Rolle in Bezug auf die Laufzeit des Algorithmus spielt. Schritt 1 zur Implementierung der Funktion \lstinline{GETNEWVISITED} ist die Verknüpfung des Bereichs des Arrays \lstinline{a[n/p*rank+1..n/p*(rank+1)}, also der Bereich für den der jeweilige Prozessor zuständig ist, mit dem Array \lstinline{v[1..n/p]} mittels binären Operation \lstinline{OR}. Mit \lstinline{n/p*rank+1} beziehungsweise \lstinline{n/p*(rank+1)} bekommt man den erforderlichen Start- und Endindex des Array \lstinline{a}, da jeder Prozessor für \(n/p\) Knoten verantwortlich ist und die Zuständigkeitsbereiche der Prozessoren geordnet nach dem Rang der Prozessoren im Array \lstinline{a} abgelegt sind. Jeder Prozessor hat einen eindeutigen Rang innerhalb der Prozessorgruppe, die alle verfügbaren Prozessoren enthält. Ränge werden vergeben von 0 bis \(p-1\).\\
Das Ergebnis der \lstinline{OR} Verknüpfung ist nun wieder eine \textit{Bitmap} der Länge \(n/p\), bei dem jene Knoten auf 1 gesetzt sind, die im letzten Level neu besucht wurden, jedoch noch nicht im aktuellen Level bearbeitet wurden, also bei denen die Nachbarn im aktuellen Level besucht werden müssen. In Schritt 2 müssen nun genau diese Knoten aus der berechneten \textit{Bitmap} herausgefiltert werden, was der \lstinline{$\textbf{for}$} Schleife in Zeile 14 in Algorithmus~\ref{par_allvisited} entspricht. Dies ist auf verschiedene Wege möglich. Eine ineffiziente Variante wäre über alle Bits zu iterieren und zu prüfen, ob dieses auf 1 gesetzt ist. Mittels der aktuellen Position der Iteration kann man auf die Identifikationsnummer des Knoten schließen.\\
Wir haben uns für eine effizientere Variante entschieden, in dem wir davon ausgehen, dass eher wenige Knoten in der aus Schritt 1 resultierenden \textit{Bitmap} auf 1 gesetzt sind, was besonders in Hinblick auf dünne Graphen der Fall ist. Wir machen Gebrauch von der in GCC (\textit{GNU Compiler Collection}) integrierten Funktion \lstinline{__builtin_clzll}, was der Funktion \lstinline{__builtin_clz} entspricht, nur dass sie auf Integer der Größe von 64 Bit operiert. Diese Funktion liefert die Anzahl der führenden Bits, die auf 0 gesetzt sind. Daraufhin kann auf die Position des Knoten geschlossen werden, der in der \textit{Bitmap} aus Schritt 2 ausgehend vom höchstwertigen Bit in Richtung niedrigstwertigen Bit als nächstes auf 1 gesetzt ist, wobei man beachten muss, dass man mit dieser Funktion immer nur auf 64 Bit der \textit{Bitmap} operieren kann. Damit liefert \lstinline{__builtin_clzll} mathematisch nichts anderes als den Logarithmus zur Basis 2 einer 64 Bit Zahl. Nachdem man die Position berechnet hat, wird der jeweilige Knoten bearbeitet und das Bit dieses Knoten auf 0 gesetzt. Schritt 2 wird solange wiederholt, bis alle Bits in der \textit{Bitmap} auf 0 gesetzt sind.\\
Die Verwendung von \textit{Bitmaps} hat nicht nur den Vorteil, dass man die Information über besucht oder nicht besucht effizienter abspeichern kann, sondern auch dass man innerhalb der \lstinline{ALLREDUCE} Operation des Arrays \lstinline{a} in Algorithmus~\ref{par_allvisited} weniger Daten zwischen den Prozessoren kommunizieren muss. In unserer Implementierung wird das bewerkstelligt durch die von MPI zur Verfügung gestellte Operation \lstinline{MPI_Allreduce} und der \textit{Bitmap}, die das Array \lstinline{a} ersetzt. Weiters wichtig für diese Implementierung ist, dass jeder MPI Prozess auf genau einem Prozessor beziehungsweise Kern eines Prozessors der verwendeten DMM ausgeführt wird. DMMs bestehen aus mehreren Verarbeitungseinheiten (Knoten) und einem Verbindungsnetzwerk. Ein Knoten ist dabei ein selbständige Einheit, die aus einem oder mehreren Prozessoren beziehungsweise Prozessorkernen besteht. Damit ist es der Fall, dass wenn mehrere Kerne eines Prozessors verwendet werden, dass auch genauso viele MPI Prozesse darauf ausgeführt werden, die auch mittels \textit{Message Passing}, also MPI, miteinander interagieren beziehungsweise kommunizieren. Abbildung~\ref{fig:purempi} zeigt eine abstrakte Darstellung dieser Zuweisung von MPI Prozessen zu Prozessorkerne.
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=1\textwidth]{purempi}
        \caption{MPI}
        \label{fig:purempi}
    \end{subfigure}
   \qquad
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=1\textwidth]{hybridmasteronly}
        \caption{MPI + OpenMP}
        \label{fig:hybridmasteronly}
    \end{subfigure}
    \caption{Abbildung~\ref{fig:purempi} zeigt die Basisvariante des Algorithmus~\ref{par_allvisited}, bei der ausschließlich mittels MPI kommuniziert wird. Im Gegensatz dazu kommunizieren in Abbildung~\ref{fig:hybridmasteronly} nur die Master Threads per MPI, innerhalb eines Knoten operieren parallele Threads auf physikalisch gemeinsamen Speicher.}
\label{fig:zerlegung}
\end{figure}
\subsection{Hybrid}
Bei der Hybrid-Variante wird die Tatsache ausgenutzt, dass die Knoten der DMM Rechner mit physikalisch gemeinsamen Speicher (SMM - \textit{shared memory machine}) darstellen und deshalb eine Kommunikation mittels \textit{Message Passing} nicht unbedingt notwendig ist. Hier machen wir Gebrauch von parallel arbeitenden \textbf{Threads}, die auf einem Knoten der DMM ausgeführt werden. Wir führen also pro Knoten genau einen MPI Prozess aus und starten innerhalb dieses Knoten eine Vielzahl von Threads, die der Anzahl der Prozessorkerne entspricht. Die Threads führen dann explizit definierte parallele Bereiche aus. Abbildung~\ref{fig:hybridmasteronly} zeigt den gerade beschriebenen Sachverhalt, wodurch auch ein Vergleich zu Abbildung~\ref{fig:purempi} möglich wird.\\
In unserer Implementierung des Hybrid-Variante haben wir als parallelen Bereich die Zeilen von 12-20 aus Algorithmus~\ref{par_allvisited} gewählt. Dies entspricht der Funktion \lstinline{GETNEWVISITED} und der Iteration über die neue besuchten aber noch nicht bearbeiteten Knoten.\\
In der Hybrid Implementierung verfügt zwar jeder MPI Prozess, der einem Knoten der DMM zugewiesen ist, über einen größeren Teilgraphen \(G_{p}\) jedoch stehen diesem MPI Prozess mehrere Threads zur Abarbeitung dieses Teilgraphen zur Verfügung, die parallel ausgeführt werden. Diese Art von Vorgehensweise verfolgt den Ansatz eines \textit{$\textbf{hybrid masteronly}$} Programmiermodell, wie von Hager, Jost und Rabenseifner in \cite{hybrid} bezeichnet. Charakteristisch für dieses Programmiermodell ist, dass keine MPI Operationen innerhalb von parallelen Bereichen ausgeführt werden und dass während die Master Threads der Knoten mittels \textit{Message Passing} miteinander kommunizieren, alle anderen Threads inaktiv sind.\\
Wenn man die Hybrid-Variante mit der normalen Variante aus Kapitel~\ref{sec:allreduce_details} vergleicht, ist es der Fall, dass bei der normalen Variante der Graph \(G\) auf die MPI Prozesse aufgeteilt wird und für jedes Level jeder MPI Prozess \(p\) sequentiell den Teilgraphen \(G_{p}\) durchsucht. Im Gegensatz dazu wird bei der Hybrid-Variante zwar ebenfalls der Graph auf die MPI Prozesse aufgeteilt, jeder MPI Prozess \(p\) durchsucht jedoch den Teilgraphen \(G_{p}\) parallel, je nach Anzahl der vorhandenen Threads.
\subsection{Paralleler BFS mit All-to-all Kommunikation}
\section{Analyse der Algorithmen und Versuchsergebnisse}
\label{sec:analysechapter}
\subsection{Analyse}
\label{sec:analyse}
Der nächste wichtige Schritt ist, die Algorithmen zu analysieren und bezüglich Laufzeit und Kosten mit dem sequentiellen Algorithmus~\ref{stacks} zu vergleichen. Dieser sequentielle Algorithmus stellt einen optimalen Algorithmus dar, es lässt sich also kein Algorithmus finden, der bezüglich Laufzeit der Breitensuche besser ist.\\
Genauso wie bei Algorithmus~\ref{stacks} die \lstinline{$\textbf{while}$} Schleife von Zeile 10-19 \($O$(D)\)-mal ausgeführt wird, wird diese auch in Algorithmus~\ref{par_allvisited} von Zeile 10-22 \($O$(D)\)-mal ausgeführt. \(D\) ist der Durchmesser des Graphen. Es ist jedoch bei unserer Implementierung des Algorithmus~\ref{par_allvisited} nicht möglich, wie beim sequentiellen Algorithmus, in \(a\) Schritten, wenn \(a\) die Anzahl der neu besuchten Knoten in einem Level darstellt, über die neu besuchten Knoten eines Levels zu iterieren. Da die Information über neu besuchte Knoten in einer Bitmap der Größe \(n/p\) liegen, wobei \(n = |V(G)|\) und \(p\) die Anzahl der MPI Prozesse ist, müssen wir zumindest jeden Integer Wert der Größe von 64 Bit in der Bitmap einmal überprüfen, ob sich in diesem ein neu besuchter, aber noch nicht bearbeiteter Knoten, befindet. Da dies in jeder Iteration der Schleife von Zeile 10-22 stattfinden muss, bekommt man rein für die Überprüfung in jedem Level, ob es einen neu besuchten Knoten gibt, eine Laufzeit von \($O$(n/(wp)*D)\) für jeden MPI Prozess. Im Vergleich dazu, hat man bei der sequentiellen Implementierung eine Laufzeit von \($O$(n)\) in Bezug auf alle Levels der Breitensuche und dem Überprüfen, welche Knoten in einem Level neu besucht wurden. Diese Laufzeit lässt sich auch zeigen, indem man zeigt, dass in den Levels \(1,2,3,..,d\) im sequentiellen Algorithmus \(a_{1},a_{2},a_{3},...,a_{d}\) Knoten besucht werden und \(a_{1} + a_{2} + a_{3} + ... + a_{d} \leq n\).\\
Da die neu besuchten Knoten in einem Integer Wert der Größe von 64 Bit mithilfe der Funktion \lstinline{__builtin_clzll} berechnet werden und wir bei dieser Funktion eine Laufzeit von \($O$(log_{2}(w))\) annehmen, \(w\) steht für die Wortlänge von 64 Bit, kommen Kosten von \($O$((n/p)*log_{2}(w))\) für alle bearbeiteten Knoten eines MPI Prozesses hinzu. Schließlich werden von jedem Prozessor noch \($O$(m/p)\) Kanten betrachtet, wobei \(m = |E(G)|\). Daraus resultiert eine Gesamtlaufzeit für jeden Prozessor von \($O$(n/(wp)*D + m/p + (n/p)*log_{2}(w))\).  Die Gesamtkosten \(C_{p}(n)\) der Implementierung des Algorithmus~\ref{par_allvisited} betragen also \($O$(n/w*D + m + n*log_{2}(w))\). Diese Implementierung ist also dann kostenoptimal, wenn der Durchmesser des Graphen sehr viel kleiner ist als die Anzahl der Knoten im Graphen, also wenn gilt \(D \ll n\) und sollte auch auf solchen Graphen angewandt werden.
\subsection{Versuchsergebnisse}
\label{sec:versuche}
In diesem Kapitel werden wir die vorgestellten Algorithmen beziehungsweise deren Implementierungen umfangreich auf der DMM Jupiter der TU Wien testen und vergleichen. Jupiter stellt 36 Knoten zur Verfügung, die jeweils zwei 8-Kern 2.3 GHz AMD Opteron 6134 Prozessoren besitzen und 32 GB Arbeitsspeicher zur Verfügung haben. Daraus resultieren insgesamt 576 Prozessorkerne und ein Arbeitsspeicher von 1152 GB. Die Knoten sind durch einen Infiniband QDR Switch MT4036 miteinander verbunden. Von diesen 576 Kernen werden wir maximal 512 Kerne nutzen, wobei wir darauf achten, dass so viele Knoten der DMM ausgelastet werden wie möglich, was einer Knotenanzahl von 32 entspricht. Wir werden wie Buluc und Madduri in~\cite{buluc} das Testmodell "Flat MPI" verfolgen, bei dem ein MPI Prozess pro verfügbaren Prozessorkern ausgeführt wird. Im Gegensatz dazu wird für die Implementierung der Hybrid-Variante folgendes Testmodell verfolgt, dass ein MPI Prozess pro Knoten der DMM gestartet wird und innerhalb des Knoten mehere Threads parallel ausgeführt werden.\\
Außerdem wichtig für die Auswertung der Ergebnisse ist der verwendete Graph und wie dieser generiert wurde. Wir haben uns für Stochastische Kronecker Graphen entschieden, die eine Generalisierung des \textit{recursive matrix} (R-MAT) Modells darstellen, da man mit diesem Modell sehr große Graphen schnell generieren kann und auch eine parallele beziehungsweise verteilte Generierung überaus gut möglich ist. Diese erzeugten Graphen besitzen ebenfalls viele Eigenschaften von sogenannten \textit{real-networks}, die sich durch eine schiefe Knotengradverteilung auszeichnet. Das bedeutet, dass die meisten Knoten eines Graphen viele ein- beziehungsweise ausgehende Kanten haben und nur ein kleine Anzahl an Knoten einen sogenannten hohen Knotengrad. Diese Informationen beziehen wir aus der Arbeit von Leskovec, Chakrabarti, Kleinberg, Faloutsos und Ghahramani~\cite{kronecker} und der Arbeit von Seshadhri, Pinar und Kolda~\cite{kronecker_study} und verweisen auch auf diese Arbeiten für ausführlichere Informationen, da dies sonst den Rahmen dieser Arbeit sprengen würde. Wir setzen die Werte in der Initiator Matrix, die für die Verteilung der Kanten im Graphen verantwortlich ist, genauso wie die Parameter \(a\), \(b\), \(c\) und \(d\) des R-MAT Generator auf \(0.57, 0.19, 0.19, 0.05\) um einen Graphen mit einem sehr kleinen Durchmesser(\(\leq 10)\)  und einer schiefen Knotengradverteilung zu erhalten.\\
Aufgrund dieser schiefen Knotengradverteilung käme es im Fall, dass wir den Graphen zur parallelen Breitensuche auf die MPI Prozesse aufteilen, zu einer ungleichmäßigen Lastverteilung zwischen den MPI Prozessen, was dazu führt das ein MPI Prozess für ein Level um einiges länger braucht als ein anderer MPI Prozess. Dies würde den Speedup, den wir durch unsere parallele Lösung erreichen wollen, um einiges verschlechtern. Deshalb führen wir eine Permutation der Identifikationsnummern der Knoten durch, was wir durch den \textit{Fisher-Yates} Mischalgorithmus bewerkstelligen. Dies bewirkt, dass die Knoten mit hohen Knotengrad bezogen auf die Identifikationsnummer nicht knapp beieinander liegen, sondern gleichmäßig über alle Identifikationsnummern verteilt sind, damit sie auch gleichmäßig über die MPI Prozesse verteilt werden. Dies ist jedoch keine Garantie, dass wirklich jeder MPI Prozess gleich viele Kanten zugeordnet bekommt, auch wenn wir einen sehr guten Pseudozufallszahlengenerator verwendet haben, nämlich den Zufallsgenerator \textbf{KISS} (\textit{Keep it simple stupid}) entwickelt von Marsaglia und Zaman~\cite{kiss}. Außerdem ist es schwierig vorherzusagen, selbst wenn die MPI Prozesse gleich viele Kanten besitzen, wie viele Knoten und dazugehörige Kanten in jedem Level eines MPI Prozesses besucht werden.\\
Interessant ist noch die Validierung des Ergebnisses unserer parallelen Breitensuche. Vorgabe bei unseren Algorithmen ist immer, dass diese ein \textit{Parent Array} am Ende der Suche zur Verfügung stellen. Dabei gibt es verschiedene Möglichkeiten, wie man dieses \textit{Parent Array} auf Gültigkeit überprüft.\\
Als Compiler wurde der GNU C Compiler Version 4.4.7 und als MPI Implementierung wurde Open MPI Version 1.10.3 verwendet. Für die Kommunikation der Threads innerhalb eines Knoten bei der Hybrid-Variante haben wir die GNU OpenMP Bibliothek eingesetzt.
\clearpage
\bibliographystyle{abbrv}
\bibliography{bachelor}
\clearpage
\appendix
\section{BFS versions}
\label{sec:versions}
\lstinputlisting[firstline=138,lastline=196,caption={Version 1 - level bitmap},label=version1]{../Source/bfs_par.c}
\lstinputlisting[firstline=135,lastline=183,caption={Version 2 - level array},label=version2]{../Source/bfs_par_parentarray.c}
\lstinputlisting[firstline=153,lastline=205,caption={Version 3 - visited bitmap},label=version3]{../Source/bfs_par_allvisited_parallelsort.c}
\section{Source Code - graph500}
\label{sec:sourcecode}
\lstinputlisting[caption={BFS solution},label=bfs]{../Source/bfs_par_allvisited_parallelsort.c}
\clearpage
\lstinputlisting[caption={Kronecker Generator},label=kronecker_generator]{../Source/kronecker_generator.c}
\clearpage
\lstinputlisting[caption={project.h},label=project]{../Source/project.h}

\clearpage


\end{document}